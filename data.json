{
  "data": {
    "name": "E資格",
    "color": "#f1f5f9",
    "desc": "日本ディープラーニング協会(JDLA)が認定するエンジニア向けAI資格。応用数学・機械学習・深層学習の基礎から応用、開発運用まで幅広い知識を体系的に問う試験",
    "children": [
      {
        "name": "応用数学",
        "color": "#f59e0b",
        "desc": "深層学習の理論的基盤となる数学分野。確率・統計によるデータの不確実性の定量化、情報理論による情報量の測定、微分・線形代数による最適化計算を包含する",
        "children": [
          {
            "name": "確率・統計",
            "color": "#f59e0b",
            "desc": "データの不確実性を数学的に扱う分野。ベイズ統計による事後確率の更新、各種確率分布によるデータ生成モデル、パラメータ推定手法を含み、機械学習の理論的基盤を形成する",
            "children": [
              {
                "name": "ベイズ則",
                "color": "#f59e0b",
                "desc": "条件付き確率の関係式P(A|B)=P(B|A)P(A)/P(B)。事前知識と観測データを組み合わせて事後確率を計算する枠組みで、機械学習における推論・学習の根幹をなす",
                "children": [
                  {
                    "name": "条件付き確率",
                    "color": "#f59e0b",
                    "desc": "事象Bが発生した条件下での事象Aの発生確率P(A|B)。ベイズ推論の出発点であり、分類問題では特徴量が与えられた下でのクラス確率の計算に直接対応する"
                  },
                  {
                    "name": "事後確率",
                    "color": "#f59e0b",
                    "desc": "データ観測後に更新されたパラメータの確率分布P(θ|D)。ベイズ則により事前分布と尤度関数から計算され、パラメータの不確実性を定量的に表現する"
                  },
                  {
                    "name": "周辺尤度",
                    "color": "#f59e0b",
                    "desc": "全パラメータについて尤度を積分した値P(D)=∫P(D|θ)P(θ)dθ。モデルエビデンスとも呼ばれ、異なるモデル構造の比較・選択の基準として使用される"
                  },
                  {
                    "name": "ベイズ更新",
                    "color": "#f59e0b",
                    "desc": "新しいデータを観測するたびに事後分布を事前分布として再利用し逐次的に信念を更新する手続き。オンライン学習や逐次推定で効率的なデータ処理を実現する"
                  }
                ]
              },
              {
                "name": "ナイーブベイズ",
                "color": "#f59e0b",
                "desc": "特徴量間の条件付き独立性を仮定した確率的分類器。単純な仮定にもかかわらずテキスト分類やスパムフィルタリング等で高い実用性能を示す軽量な手法",
                "children": [
                  {
                    "name": "条件付き独立",
                    "color": "#f59e0b",
                    "desc": "クラスラベルが与えられた条件下で各特徴量が互いに独立であるという仮定。この仮定により同時確率の計算が各特徴の確率の積に分解でき、計算が大幅に簡略化される"
                  },
                  {
                    "name": "多項NB",
                    "color": "#f59e0b",
                    "desc": "特徴量が多項分布に従うと仮定するナイーブベイズ分類器。文書中の単語出現頻度をモデル化するのに適しており、テキスト分類タスクで広く使用される標準的手法"
                  },
                  {
                    "name": "ガウスNB",
                    "color": "#f59e0b",
                    "desc": "各特徴量がガウス分布に従うと仮定するナイーブベイズ分類器。連続値の特徴量に適用でき、各クラスの平均と分散を推定するだけでモデル構築が完了する軽量な手法"
                  },
                  {
                    "name": "ラプラス平滑化",
                    "color": "#f59e0b",
                    "desc": "訓練データに未出現のカテゴリに微小確率(通常α=1)を付与してゼロ確率問題を回避する手法。加法的平滑化とも呼ばれ、ナイーブベイズの実用上必須のテクニック"
                  }
                ]
              },
              {
                "name": "MAP推定",
                "color": "#f59e0b",
                "desc": "事後確率P(θ|D)を最大化するパラメータθを求める点推定法。最尤推定に事前分布の情報を加えた手法で、正則化付き最尤推定と数学的に等価な関係にある",
                "children": [
                  {
                    "name": "事前分布",
                    "color": "#f59e0b",
                    "desc": "データ観測前のパラメータに対する信念を表す確率分布。無情報事前分布から情報事前分布まで様々な選択があり、ベイズ推定の結果に大きく影響する重要な設計要素"
                  },
                  {
                    "name": "正則化との関連",
                    "color": "#f59e0b",
                    "desc": "MAP推定でガウス事前分布を用いるとL2正則化、ラプラス事前分布を用いるとL1正則化と数学的に等価になる。ベイズ的視点から正則化の意味づけを与える重要な関係性"
                  },
                  {
                    "name": "ガウス事前分布",
                    "color": "#f59e0b",
                    "desc": "パラメータが平均0の正規分布N(0,σ²)に従うと仮定する事前分布。MAP推定ではパラメータの二乗和ペナルティ(L2正則化)に対応し、パラメータを小さく保つ効果がある"
                  }
                ]
              },
              {
                "name": "最尤推定(MLE)",
                "color": "#f59e0b",
                "desc": "観測データの尤度関数L(θ)=P(D|θ)を最大化するパラメータθを求める推定法。統計学・機械学習で最も基本的な推定手法で、大標本では一致性と漸近正規性を持つ",
                "children": [
                  {
                    "name": "対数尤度",
                    "color": "#f59e0b",
                    "desc": "尤度関数の対数logL(θ)。独立なデータの尤度の積が和に変換されるため数値的安定性と計算効率が向上する。最適化においてはMLEの目的関数として直接使用される"
                  },
                  {
                    "name": "尤度関数",
                    "color": "#f59e0b",
                    "desc": "パラメータθの関数として見たデータの生成確率L(θ)=P(D|θ)。確率とは異なりθの関数であり、どのパラメータ値がデータをよく説明するかを定量的に評価する"
                  },
                  {
                    "name": "十分統計量",
                    "color": "#f59e0b",
                    "desc": "パラメータ推定に必要な情報を過不足なく持つデータの要約統計量。例えばガウス分布では標本平均と標本分散が十分統計量であり、データ全体を保持する必要がなくなる"
                  },
                  {
                    "name": "フィッシャー情報量",
                    "color": "#f59e0b",
                    "desc": "対数尤度の二階微分の期待値の負値で定義される情報量。パラメータ推定の精度の理論的限界(クラメール・ラオの下界)を決定し、推定量の効率性評価に使用される"
                  }
                ]
              },
              {
                "name": "ベイズ推定・推論",
                "color": "#f59e0b",
                "desc": "パラメータの事後分布全体を計算して不確実性を定量化する統計的推論の枠組み。点推定だけでなく予測分布の計算やモデル選択にも応用される包括的なアプローチ",
                "children": [
                  {
                    "name": "ベイズ推定",
                    "color": "#f59e0b",
                    "desc": "事後分布P(θ|D)全体を求める推定法。パラメータの点推定ではなく分布として不確実性を表現できるため、データが少ない場合でも信頼性の高い予測と意思決定が可能になる"
                  },
                  {
                    "name": "ベイズ推論",
                    "color": "#f59e0b",
                    "desc": "事後分布を用いて未知の量について確率的推論を行う過程。新規データの予測分布∫P(x|θ)P(θ|D)dθの計算や仮説検定など、不確実性を考慮した意思決定に使用される"
                  },
                  {
                    "name": "ベイズ最適化",
                    "color": "#f59e0b",
                    "desc": "ガウス過程等の代理モデルと獲得関数を用いてブラックボックス関数を少ない評価回数で効率的に最適化する手法。NNのハイパーパラメータ探索で特に有効な手法として知られる"
                  },
                  {
                    "name": "獲得関数",
                    "color": "#f59e0b",
                    "desc": "ベイズ最適化で次に評価すべき点を決定する関数。EI(期待改善量)・UCB(上側信頼限界)・PI(改善確率)等があり、探索(未知領域)と活用(高性能領域)のバランスを制御する"
                  }
                ]
              },
              {
                "name": "共役事前分布",
                "color": "#f59e0b",
                "desc": "事後分布が事前分布と同じ分布族に属するような事前分布の選択。ベータ-二項・ガウス-ガウス等のペアがあり、事後分布を解析的に閉じた形で計算できる利点がある",
                "children": [
                  {
                    "name": "ベータ分布",
                    "color": "#f59e0b",
                    "desc": "二項分布の共役事前分布で、区間[0,1]上の確率パラメータの分布を表す。形状パラメータα,βにより一様・U字型・山型など多様な分布形状を表現でき、ベイズ推定で頻用される"
                  },
                  {
                    "name": "ディリクレ分布",
                    "color": "#f59e0b",
                    "desc": "多項分布の共役事前分布で、カテゴリ確率ベクトルの分布を表すベータ分布の多変量拡張。LDAなどのトピックモデルや言語モデルの事前分布として自然言語処理でも重要"
                  },
                  {
                    "name": "解析的計算",
                    "color": "#f59e0b",
                    "desc": "MCMCなどの数値近似に頼らず、事後分布を閉じた数式で厳密に計算できること。共役事前分布を用いる主要な利点であり、計算効率と解の正確性の両方を保証する"
                  }
                ]
              },
              {
                "name": "確率分布",
                "color": "#f59e0b",
                "desc": "確率変数の取りうる値とその確率の対応関係を定める数学的関数。ガウス・ベルヌーイ・多項・ポアソン等があり、データの生成過程をモデル化する際の基本的な構成要素となる",
                "children": [
                  {
                    "name": "ガウス分布",
                    "color": "#f59e0b",
                    "desc": "平均μと分散σ²の2パラメータで完全に特徴づけられる連続確率分布。中心極限定理により自然界で広く出現し、正規分布とも呼ばれる統計学・機械学習の最重要分布"
                  },
                  {
                    "name": "ベルヌーイ",
                    "color": "#f59e0b",
                    "desc": "成功(1)か失敗(0)の二値をとる最も単純な確率分布。パラメータpが成功確率を表す。コイン投げのモデルであり、二値分類やロジスティック回帰の理論的基盤となる"
                  },
                  {
                    "name": "多項分布",
                    "color": "#f59e0b",
                    "desc": "k個のカテゴリへのn回の試行結果の分布。ベルヌーイ分布の多変量拡張であり、テキスト分類における単語のBag-of-Words表現やSoftmax出力の確率モデルとして使用される"
                  },
                  {
                    "name": "ポアソン",
                    "color": "#f59e0b",
                    "desc": "単位時間・単位面積あたりの事象発生回数をモデル化する離散確率分布。パラメータλが平均発生率を表し、稀な事象の発生回数のモデリングに適している"
                  },
                  {
                    "name": "指数分布族",
                    "color": "#f59e0b",
                    "desc": "ガウス・ベルヌーイ・ポアソン・ガンマ等を統一的な数式exp(η·T(x)-A(η))で表現できる分布の族。十分統計量が存在し一般化線形モデル(GLM)の理論的基盤となる"
                  }
                ]
              },
              {
                "name": "パラメータ推定",
                "color": "#f59e0b",
                "desc": "観測データからモデルのパラメータを決定する統計的手法の総称。最尤推定(MLE)・MAP推定・ベイズ推定の3つが代表的で、データ量や事前知識に応じて使い分ける",
                "children": [
                  {
                    "name": "MLE",
                    "color": "#f59e0b",
                    "desc": "尤度関数L(θ)=P(D|θ)を最大化してパラメータの点推定を行う方法。事前情報を使わず純粋にデータのみから推定する最も基本的な手法で、大標本では最適な推定量となる"
                  },
                  {
                    "name": "MAP",
                    "color": "#f59e0b",
                    "desc": "事後確率P(θ|D)∝P(D|θ)P(θ)を最大化するパラメータを求める推定法。MLEに事前分布という追加情報を加えることで、データが少ない場合の推定を安定化させる"
                  },
                  {
                    "name": "推定量",
                    "color": "#f59e0b",
                    "desc": "データからパラメータを推定するための関数(統計量)。不偏性(期待値が真値に一致)・一致性(データ増加で真値に収束)・有効性(分散が最小)等の統計的性質で評価される"
                  }
                ]
              }
            ]
          },
          {
            "name": "情報理論",
            "color": "#f59e0b",
            "desc": "情報の定量化・伝送・圧縮を扱う数学理論。Shannonが1948年に確立。エントロピー・KLダイバージェンス・相互情報量等の概念は深層学習の損失関数設計に直結する",
            "children": [
              {
                "name": "エントロピー",
                "color": "#f59e0b",
                "desc": "確率分布の不確実性・ランダムさをH(X)=-Σ P(x)log P(x)で定量化する指標。一様分布で最大、確定的分布で0となる。情報理論と機械学習をつなぐ中心概念",
                "children": [
                  {
                    "name": "自己情報量",
                    "color": "#f59e0b",
                    "desc": "事象xの発生に伴う情報量I(x)=-log P(x)。確率が低い(意外な)事象ほど大きな情報量を持つ。エントロピーは自己情報量の期待値として定義される"
                  },
                  {
                    "name": "条件付きエントロピー",
                    "color": "#f59e0b",
                    "desc": "確率変数Xを知った上でのYの残存不確実性H(Y|X)=Σ_x P(x)H(Y|X=x)。相互情報量I(X;Y)=H(Y)-H(Y|X)の計算に使用され、特徴量の情報価値の評価に有用"
                  },
                  {
                    "name": "結合エントロピー",
                    "color": "#f59e0b",
                    "desc": "2つの確率変数の同時分布のエントロピーH(X,Y)。H(X,Y)=H(X)+H(Y|X)=H(Y)+H(X|Y)の関係が成り立ち、独立のときH(X,Y)=H(X)+H(Y)で最大となる"
                  },
                  {
                    "name": "エントロピー最小化",
                    "color": "#f59e0b",
                    "desc": "モデルの出力分布の不確実性を最小化する最適化目的。交差エントロピー損失の最小化は、真の分布に対する予測分布のエントロピー最小化と等価な関係にある"
                  }
                ]
              },
              {
                "name": "交差エントロピー",
                "color": "#f59e0b",
                "desc": "真の分布pとモデル分布qの間のH(p,q)=-Σp(x)log q(x)。多クラス分類の標準的な損失関数で、KLダイバージェンスとエントロピーの和H(p,q)=H(p)+D_KL(p||q)に分解される",
                "children": [
                  {
                    "name": "負の対数尤度",
                    "color": "#f59e0b",
                    "desc": "交差エントロピー損失が最尤推定の目的関数-log P(D|θ)と数学的に等価であること。分類における交差エントロピー最小化は、正解ラベルの対数尤度を最大化することに他ならない"
                  },
                  {
                    "name": "Softmax+CE",
                    "color": "#f59e0b",
                    "desc": "多クラス分類の出力層でSoftmax関数を適用し交差エントロピー損失で学習する標準的構成。数値安定性のためlog-sum-exp trickを用いた一体計算が実装上重要となる"
                  },
                  {
                    "name": "Focal Loss",
                    "color": "#f59e0b",
                    "desc": "交差エントロピーに(1-p_t)^γの重みを付けて簡単なサンプルの寄与を低減する損失関数。γ≥1で難しいサンプルに学習を集中させ、物体検出等のクラス不均衡問題に効果的"
                  },
                  {
                    "name": "クロスエントロピー誤差",
                    "color": "#f59e0b",
                    "desc": "真の分布pとモデル出力qの交差エントロピーH(p,q)=-Σp(x)log q(x)。分類問題の標準損失関数で、正解クラスの予測確率が高いほど損失が小さくなる性質を持つ"
                  }
                ]
              },
              {
                "name": "相互情報量",
                "color": "#f59e0b",
                "desc": "2つの確率変数の依存関係の強さI(X;Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)。独立のとき0、完全依存で最大。特徴量選択や表現学習(InfoMax原理)で使用される重要な情報量",
                "children": [
                  {
                    "name": "データ処理不等式",
                    "color": "#f59e0b",
                    "desc": "X→Y→Zのマルコフ連鎖でI(X;Z)≤I(X;Y)が成り立つ原理。データ処理(変換)によって情報は増えないことを示し、表現学習やボトルネック理論の理論的基盤となる"
                  },
                  {
                    "name": "InfoMax原理",
                    "color": "#f59e0b",
                    "desc": "ニューラルネットワークの各層が入力と出力の相互情報量を最大化するように学習すべきとする原理。対照学習(SimCLR等)の理論的根拠であり、自己教師あり学習の基盤"
                  }
                ]
              },
              {
                "name": "ダイバージェンス",
                "color": "#f59e0b",
                "desc": "2つの確率分布間の差異を測る指標の総称。KL・JS・Wasserstein・f-divergence等があり、それぞれ異なる数学的性質を持つ。生成モデルの損失関数設計で重要な選択要素",
                "children": [
                  {
                    "name": "KLダイバージェンス",
                    "color": "#f59e0b",
                    "desc": "2つの確率分布P,Q間の非対称的な距離的指標D_KL(P||Q)=Σ P(x)log(P(x)/Q(x))。分布の近さを測る基本的な道具で、変分推論やVAEの目的関数の構成要素",
                    "children": [
                      {
                        "name": "非対称性",
                        "color": "#f59e0b",
                        "desc": "D_KL(P||Q)≠D_KL(Q||P)が一般に成り立つKLダイバージェンスの重要な性質。この非対称性により順方向KLと逆方向KLで異なる近似の性質が生じ、用途に応じた使い分けが必要"
                      },
                      {
                        "name": "Forward KL",
                        "color": "#f59e0b",
                        "desc": "D_KL(P||Q)の形式。真の分布Pが正の確率を持つ全領域でQもカバーする(mode-covering)傾向があり、分布全体を平均的に近似する。変分推論ではあまり使われない"
                      },
                      {
                        "name": "Reverse KL",
                        "color": "#f59e0b",
                        "desc": "D_KL(Q||P)の形式。近似分布Qが集中する領域で真の分布Pと一致する(mode-seeking)傾向があり、特定のモードに集中した近似を行う。変分推論の標準的な目的関数"
                      },
                      {
                        "name": "ELBO",
                        "color": "#f59e0b",
                        "desc": "Evidence Lower Bound(変分下界)。対数周辺尤度log P(D)の下界で、ELBO=E[log P(D|z)]-D_KL(Q(z)||P(z))と分解される。VAEや変分推論の最適化目的関数として中心的な役割"
                      }
                    ]
                  },
                  {
                    "name": "JSダイバージェンス",
                    "color": "#f59e0b",
                    "desc": "KLダイバージェンスを対称化した距離指標JS(P||Q)=(KL(P||M)+KL(Q||M))/2、M=(P+Q)/2。値域が[0,log2]の有界な指標で、元祖GANの損失関数と理論的に等価",
                    "children": [
                      {
                        "name": "対称性",
                        "color": "#f59e0b",
                        "desc": "JS(P||Q)=JS(Q||P)が成り立つ性質。KLダイバージェンスの非対称性の問題を解消し、2つの分布間の距離として直感的に扱いやすい指標となっている"
                      },
                      {
                        "name": "有界",
                        "color": "#f59e0b",
                        "desc": "JSダイバージェンスの値が常に[0, log2]の範囲に収まる性質。KLが無限大になりうるのと対照的で、数値的に安定した最適化が可能。ただしGAN学習では勾配消失の原因にもなる"
                      },
                      {
                        "name": "GAN目的関数",
                        "color": "#f59e0b",
                        "desc": "元祖GANのmin-maxゲームの最適解が2·JS(P_data||P_g)-log4で表されること。生成分布と真のデータ分布のJS距離を最小化していることを意味する"
                      },
                      {
                        "name": "KLの対称化",
                        "color": "#f59e0b",
                        "desc": "JS(P||Q)=(KL(P||M)+KL(Q||M))/2（M=(P+Q)/2）としてKLダイバージェンスを対称化する操作。混合分布Mを介することで両方向のKLを等しく考慮した距離指標を構成する"
                      }
                    ]
                  },
                  {
                    "name": "Wasserstein",
                    "color": "#f59e0b",
                    "desc": "確率分布間の最適輸送コストに基づく距離。Earth Mover's距離とも呼ばれる。JS距離と異なり分布の台が重ならなくても有意な勾配を持ち、WGANの損失関数として採用"
                  },
                  {
                    "name": "f-divergence",
                    "color": "#f59e0b",
                    "desc": "凸関数fを用いてD_f(P||Q)=Σ Q(x)f(P(x)/Q(x))で定義されるダイバージェンスの一般的枠組み。f(t)=tlog tでKL、f(t)=-(t+1)log((t+1)/2)+tlog tでJSが特殊ケース"
                  }
                ]
              }
            ]
          },
          {
            "name": "微分・線形代数",
            "color": "#f59e0b",
            "desc": "深層学習の最適化と演算を支える数学的基盤。連鎖律による誤差逆伝播、自動微分によるフレームワーク実装、行列演算による効率的な順伝播計算を提供する",
            "children": [
              {
                "name": "連鎖律",
                "color": "#f59e0b",
                "desc": "合成関数y=f(g(x))の微分をdy/dx=(dy/dg)·(dg/dx)で計算する法則。多層NNにおける誤差逆伝播法の数学的基盤であり、各層の局所勾配の積で全体の勾配を求める",
                "children": [
                  {
                    "name": "誤差逆伝播",
                    "color": "#f59e0b",
                    "desc": "連鎖律を適用して出力層の損失から各層のパラメータ勾配を逆方向に効率的に計算するアルゴリズム。深層学習の学習を実現する核心技術で、計算量は順伝播と同程度"
                  },
                  {
                    "name": "計算グラフ",
                    "color": "#f59e0b",
                    "desc": "数式の各演算をノード、データの流れをエッジで表現した有向グラフ。順方向でデータを伝播し逆方向で勾配を伝播させる自動微分の実装基盤。PyTorch等で動的に構築される"
                  }
                ]
              },
              {
                "name": "自動微分",
                "color": "#f59e0b",
                "desc": "計算グラフを記録・追跡してプログラムの微分を自動的に計算するフレームワーク機能。手動での勾配導出が不要になり、複雑なモデルでも正確な勾配を効率的に計算できる",
                "children": [
                  {
                    "name": "autograd",
                    "color": "#f59e0b",
                    "desc": "PyTorchの自動微分エンジン。Tensorの演算を動的に計算グラフとして記録し、.backward()呼び出しで全パラメータの勾配を自動計算する。Define-by-Runの中核機能"
                  },
                  {
                    "name": "GradientTape",
                    "color": "#f59e0b",
                    "desc": "TensorFlowのEager実行モードにおける自動微分API。with tf.GradientTape()ブロック内の演算を記録し、tape.gradient()で任意の変数に対する勾配を計算する"
                  },
                  {
                    "name": "Forward/Reverse mode",
                    "color": "#f59e0b",
                    "desc": "自動微分の2方式。Forward modeは入力数が少ない場合、Reverse modeは出力数が少ない場合に効率的。NNは出力(損失)が1つなのでReverse mode(=逆伝播)が標準"
                  }
                ]
              },
              {
                "name": "偏微分",
                "color": "#f59e0b",
                "desc": "多変数関数f(x₁,...,xₙ)の特定の変数xᵢのみで微分し他を定数として扱う操作。勾配ベクトル∇f=(∂f/∂x₁,...,∂f/∂xₙ)の各成分に対応し、最適化の方向決定に使用される"
              },
              {
                "name": "順伝播",
                "color": "#f59e0b",
                "desc": "入力層から出力層へ各層の線形変換と活性化関数を順次適用してデータを伝播させる計算過程。推論時のフォワードパスであり、計算グラフの順方向の走査に対応する"
              },
              {
                "name": "逆伝播",
                "color": "#f59e0b",
                "desc": "出力層の損失から入力層へ向かって連鎖律に基づき各パラメータの勾配を逆方向に伝播させる過程。計算グラフの逆走査で実現され、パラメータ更新に必要な勾配を効率的に求める"
              }
            ]
          }
        ]
      },
      {
        "name": "機械学習",
        "color": "#10b981",
        "desc": "データからパターンを自動的に学習しタスクの性能を向上させるアルゴリズムの総称。教師あり・教師なし・半教師あり学習の枠組みがあり、パターン認識やモデル評価手法を含む",
        "children": [
          {
            "name": "教師あり学習",
            "color": "#10b981",
            "desc": "入力xと正解ラベルyのペア(x,y)からx→yの写像を学習する枠組み。回帰(連続値予測)と分類(離散ラベル予測)に大別され、最も広く使用される機械学習パラダイム",
            "children": [
              {
                "name": "回帰",
                "color": "#10b981",
                "desc": "連続値の出力を予測するタスク。住宅価格・気温・売上等の予測が代表例。損失関数にMSEを使用し、線形回帰からニューラルネットワークまで多様なモデルが適用される",
                "children": [
                  {
                    "name": "線形回帰",
                    "color": "#10b981",
                    "desc": "y=Xw+bの線形モデルで出力を予測する最も基本的な回帰手法。最小二乗法で解析的に解が求まり、解釈性が高い。全ての回帰モデルの基礎として理論的に重要"
                  },
                  {
                    "name": "Ridge",
                    "color": "#10b981",
                    "desc": "L2正則化項λ||w||²を加えた線形回帰。パラメータの大きさを抑制して多重共線性を緩和し過学習を防ぐ。正則化の強さλはバイアス-バリアンスのトレードオフを制御する"
                  },
                  {
                    "name": "Lasso",
                    "color": "#10b981",
                    "desc": "L1正則化項λ||w||₁を加えた線形回帰。不要な特徴量の係数を厳密にゼロにするスパースな解を得られるため、特徴量選択の効果がある。高次元データで特に有効"
                  },
                  {
                    "name": "順序回帰",
                    "color": "#10b981",
                    "desc": "低/中/高のように順序関係のあるカテゴリを予測する回帰手法。通常の回帰とも多クラス分類とも異なり、閾値モデルや累積リンクモデル等で順序制約を考慮して定式化される"
                  },
                  {
                    "name": "最小二乗法",
                    "color": "#10b981",
                    "desc": "残差の二乗和Σ(yᵢ-ŷᵢ)²を最小化してパラメータを求める方法。線形回帰では正規方程式X^TXw=X^Tyで解析的に解が求まる。ガウスノイズ下でMLEと等価になる"
                  }
                ]
              },
              {
                "name": "分類",
                "color": "#10b981",
                "desc": "離散的なクラスラベルを予測するタスク。メール分類・画像認識・疾病診断等が代表例。決定境界を学習し、ロジスティック回帰・SVM・決定木等の多様なモデルが使用される",
                "children": [
                  {
                    "name": "ロジスティック回帰",
                    "color": "#10b981",
                    "desc": "線形結合の出力にシグモイド関数σ(z)=1/(1+e^(-z))を適用して[0,1]の確率を出力する二値分類モデル。交差エントロピーで最適化し、解釈性の高い基本的分類器"
                  },
                  {
                    "name": "SVM",
                    "color": "#10b981",
                    "desc": "サポートベクターマシン。クラス間のマージン(決定境界と最近接データ点の距離)を最大化する分類器。カーネルトリックで非線形分類に拡張可能。汎化性能の理論的保証がある"
                  },
                  {
                    "name": "決定木",
                    "color": "#10b981",
                    "desc": "特徴量の条件分岐を木構造で表現して予測する手法。情報利得やジニ不純度で最適な分割を選択する。解釈性が非常に高く、アンサンブル手法の基本構成要素としても重要"
                  },
                  {
                    "name": "多クラス分類",
                    "color": "#10b981",
                    "desc": "3つ以上のクラスへの分類タスク。One-vs-All戦略で二値分類器を組合せる方法と、Softmax出力で直接多クラス確率を出力する方法がある。交差エントロピー損失で学習"
                  },
                  {
                    "name": "マルチラベル分類",
                    "color": "#10b981",
                    "desc": "1つのサンプルに複数のラベルを同時に付与する分類タスク。画像タグ付け等が例。各ラベルを独立した二値分類として扱い、バイナリクロスエントロピーで学習するのが一般的"
                  }
                ]
              }
            ]
          },
          {
            "name": "教師なし学習",
            "color": "#10b981",
            "desc": "正解ラベルなしのデータからパターンや構造を発見する学習枠組み。クラスタリングによるグループ化、次元削減による可視化・圧縮、異常検出による外れ値発見等を含む",
            "children": [
              {
                "name": "クラスタリング",
                "color": "#10b981",
                "desc": "ラベルなしデータを類似度に基づいてグループ(クラスタ)に自動分割する手法。k-means・階層的クラスタリング・DBSCAN等があり、顧客セグメンテーション等に応用される",
                "children": [
                  {
                    "name": "k-means",
                    "color": "#10b981",
                    "desc": "データをk個のクラスタに分割するアルゴリズム。(1)重心初期化→(2)最近傍割当→(3)重心再計算を収束まで反復する。シンプルで高速だが初期値とkの選択に敏感"
                  },
                  {
                    "name": "k-means++",
                    "color": "#10b981",
                    "desc": "k-meansの初期重心選択を改良した手法。既存重心から遠い点を確率的に優先選択することで、悪い初期値による局所最適解を回避し収束速度と最終品質を大幅に改善する"
                  },
                  {
                    "name": "エルボー法",
                    "color": "#10b981",
                    "desc": "クラスタ数kを決定する手法。kを増やしながらクラスタ内距離の和(歪み)を計算し、そのグラフにおいて減少率が急に緩やかになる「肘」の位置を最適なkとして選択する"
                  },
                  {
                    "name": "半教師ありクラスタリング",
                    "color": "#10b981",
                    "desc": "少数のラベル付きデータの情報を制約として活用し、教師なしクラスタリングの精度を向上させる手法。Must-link/Cannot-link制約等でドメイン知識を反映する"
                  }
                ]
              },
              {
                "name": "次元削減",
                "color": "#10b981",
                "desc": "高次元データを情報損失を最小限に抑えて低次元空間に射影する手法。PCA等の線形手法とt-SNE等の非線形手法があり、可視化・ノイズ除去・計算効率向上に使用される",
                "children": [
                  {
                    "name": "主成分分析(PCA)",
                    "color": "#10b981",
                    "desc": "データの分散が最大となる直交方向(主成分)を見つけ、そこに射影する線形次元削減法。共分散行列の固有値分解で実現し、累積寄与率で次元数を決定する標準的手法"
                  },
                  {
                    "name": "LSI",
                    "color": "#10b981",
                    "desc": "Latent Semantic Indexing。文書-単語行列にSVD(特異値分解)を適用して次元削減し、単語の共起パターンから潜在的な意味構造を抽出する情報検索・自然言語処理の手法"
                  },
                  {
                    "name": "潜在的意味インデキシング",
                    "color": "#10b981",
                    "desc": "LSIの正式な日本語名称。文書と単語の関係を潜在意味空間で捉え、同義語や多義語による語彙のミスマッチ問題を緩和する。トピックモデルの先駆的手法"
                  },
                  {
                    "name": "t-SNE",
                    "color": "#10b981",
                    "desc": "t-distributed Stochastic Neighbor Embedding。高次元データの局所的な近傍関係を保持しつつ2D/3Dに非線形写像する可視化特化の手法。クラスタ構造の視覚的把握に優れる"
                  }
                ]
              },
              {
                "name": "異常検出",
                "color": "#10b981",
                "desc": "データ分布から大きく逸脱したサンプル(外れ値・異常)を検出するタスク。製造業の品質管理・不正検知・医療診断等に応用。正常データのみで学習する半教師あり的アプローチが主流",
                "children": [
                  {
                    "name": "再構成誤差",
                    "color": "#10b981",
                    "desc": "オートエンコーダ等で入力を圧縮・復元した際の誤差。正常データで学習したモデルは異常データの再構成が困難になるため、誤差の大きさを異常度のスコアとして利用する"
                  },
                  {
                    "name": "外れ値",
                    "color": "#10b981",
                    "desc": "データの大多数から統計的に大きく逸脱したサンプル。箱ひげ図のIQR法・Zスコア・LOF(局所外れ値因子)・Isolation Forest等の手法で検出される"
                  },
                  {
                    "name": "正常分布学習",
                    "color": "#10b981",
                    "desc": "正常データのみの分布を学習し、そこからの逸脱度合いで異常を検出する枠組み。One-Class SVM・Deep SVDD・自己符号化器等の手法があり、異常データが稀少な実応用に適する"
                  }
                ]
              }
            ]
          },
          {
            "name": "半教師あり学習",
            "color": "#10b981",
            "desc": "少数のラベル付きデータと大量のラベルなしデータを併用して学習する枠組み。ラベル付けコストの削減が目的。自己訓練・共訓練・一致性正則化等の手法がある",
            "children": [
              {
                "name": "自己訓練",
                "color": "#10b981",
                "desc": "モデルの高信頼度な予測をラベルなしデータの擬似ラベルとして採用し、訓練データを拡張して再学習する反復的手法。実装が簡単で広く使われるが誤りの蓄積に注意が必要",
                "children": [
                  {
                    "name": "擬似ラベル",
                    "color": "#10b981",
                    "desc": "モデルの予測結果をラベルなしデータの仮のラベルとして利用する手法。高信頼度の予測のみを採用することで品質を担保する。Deep Learningでは確率的な閾値フィルタリングが一般的"
                  },
                  {
                    "name": "反復学習",
                    "color": "#10b981",
                    "desc": "擬似ラベルの付与→モデル再学習→新たな擬似ラベル生成を繰り返し、モデルの精度を段階的に向上させる自己訓練のプロセス。収束条件や誤り蓄積の制御が設計上の課題"
                  },
                  {
                    "name": "信頼度閾値",
                    "color": "#10b981",
                    "desc": "擬似ラベルを採用する際のモデル予測確信度の下限値。閾値が高いほど品質は高いが利用データ量が減少する。カリキュラム学習的に閾値を徐々に下げる戦略も効果的"
                  }
                ]
              },
              {
                "name": "共訓練",
                "color": "#10b981",
                "desc": "データの2つの異なるビュー(特徴量セット)で独立に学習した2つのモデルが、互いの高信頼予測を相手の訓練データに追加し合う協調学習法。ビューの条件付き独立性を仮定",
                "children": [
                  {
                    "name": "マルチビュー",
                    "color": "#10b981",
                    "desc": "同一データを異なる視点・特徴量セットで表現すること。テキスト+画像、音声+映像等。共訓練の前提条件であり、各ビューが独立に十分な情報を持つことが理想的"
                  },
                  {
                    "name": "相互ラベリング",
                    "color": "#10b981",
                    "desc": "2つのモデルが互いの高信頼度な予測を相手の訓練データとして追加し合う学習方式。1つのモデルの偏りをもう1つが補正する効果があり、共訓練の核心的メカニズム"
                  }
                ]
              },
              {
                "name": "一致性正則化",
                "color": "#10b981",
                "desc": "入力データに摂動(ノイズ・拡張)を加えても出力が一貫することを要求する正則化手法。ラベルなしデータに適用でき、決定境界を低密度領域に誘導する効果がある",
                "children": [
                  {
                    "name": "摂動",
                    "color": "#10b981",
                    "desc": "入力データに加える小さなノイズや変換(回転・反転・ドロップアウト等)。一致性正則化では摂動前後の出力の一致を要求することで、モデルのロバスト性と汎化性能を向上させる"
                  },
                  {
                    "name": "出力一貫性",
                    "color": "#10b981",
                    "desc": "摂動された入力に対してもモデルの出力確率分布が安定的であること。この性質を損失関数として明示的に学習させることで、半教師あり学習の性能が大幅に向上する"
                  },
                  {
                    "name": "MixMatch",
                    "color": "#10b981",
                    "desc": "擬似ラベル付与・一致性正則化・MixUpデータ拡張・エントロピー最小化を統合した半教師あり学習の包括的手法。各要素の相乗効果で当時のSOTAを大幅に更新した"
                  }
                ]
              }
            ]
          },
          {
            "name": "パターン認識",
            "color": "#10b981",
            "desc": "距離関数に基づくデータの分類・識別手法の体系。k近傍法による非パラメトリック分類、各種距離関数による類似度計算、kd-tree等による高速近傍探索を含む",
            "children": [
              {
                "name": "k近傍法",
                "color": "#10b981",
                "desc": "テストデータのk個の最近傍の訓練データのラベルで多数決分類する非パラメトリック手法。モデル構築不要(怠惰学習)で直感的だが、推論時に全データとの距離計算が必要",
                "children": [
                  {
                    "name": "怠惰学習",
                    "color": "#10b981",
                    "desc": "訓練フェーズでモデルを明示的に構築せず、推論時に全訓練データを参照して予測する学習方式。kNNが代表例。事前の学習コストが零だが推論時の計算量が大きい"
                  },
                  {
                    "name": "重み付きkNN",
                    "color": "#10b981",
                    "desc": "距離が近い近傍サンプルほど大きな投票重みを与えるkNN。重みとして距離の逆数1/dを用いることが多く、遠くのノイズ的な近傍の影響を抑制して予測精度を向上させる"
                  },
                  {
                    "name": "Voronoi図",
                    "color": "#10b981",
                    "desc": "各データ点を最も近い参照点で領域分割した空間分割図。1-NNの決定境界に対応し、k近傍法の幾何学的な解釈を与える。計算幾何学における基本的なデータ構造"
                  }
                ]
              },
              {
                "name": "距離関数",
                "color": "#10b981",
                "desc": "2つのデータ点間の類似度・相違度を定量化する関数。ユークリッド・コサイン・マンハッタン等があり、データの特性とタスクに応じた適切な距離関数の選択がkNN等の性能を左右する",
                "children": [
                  {
                    "name": "ユークリッド距離",
                    "color": "#10b981",
                    "desc": "2点間の直線距離d=√Σ(xᵢ-yᵢ)²。L2ノルムに対応する最も一般的な距離尺度。等方的な空間で自然な距離だが、高次元では次元の呪いの影響を受けやすい"
                  },
                  {
                    "name": "コサイン距離",
                    "color": "#10b981",
                    "desc": "ベクトル間の角度に基づく距離1-cos(θ)。ベクトルの大きさではなく方向の類似性を測るため、文書ベクトルや単語埋め込み等の高次元スパースデータの類似度計算に適する"
                  },
                  {
                    "name": "マンハッタン距離",
                    "color": "#10b981",
                    "desc": "各座標の差の絶対値の合計d=Σ|xᵢ-yᵢ|。L1ノルムに対応。格子状の道路を移動する距離に相当し、外れ値に対してユークリッド距離より頑健な性質を持つ"
                  },
                  {
                    "name": "Lp距離",
                    "color": "#10b981",
                    "desc": "ミンコフスキー距離d=(Σ|xᵢ-yᵢ|^p)^(1/p)の一般形。p=1でマンハッタン、p=2でユークリッド、p→∞でチェビシェフ距離に対応する距離関数の統一的枠組み"
                  },
                  {
                    "name": "マハラノビス距離",
                    "color": "#10b981",
                    "desc": "データの分散・共分散構造を考慮した距離d=√((x-μ)ᵀΣ⁻¹(x-μ))。特徴量間の相関と分散の違いを正規化するため、異方的な分布での異常検出や分類に特に有効"
                  }
                ]
              },
              {
                "name": "kd-tree",
                "color": "#10b981",
                "desc": "k次元空間を再帰的に二分割する木構造データ構造。最近傍探索をO(log n)で実現するが、高次元(20次元超)では次元の呪いにより効率が低下し線形探索と同等になる",
                "children": [
                  {
                    "name": "次元の呪い",
                    "color": "#10b981",
                    "desc": "特徴量の次元が増加すると空間の体積が指数的に拡大し、データが極端にスパースになる現象。距離ベースの手法の判別力が低下し、必要なデータ量が指数的に増大する"
                  },
                  {
                    "name": "近似最近傍探索",
                    "color": "#10b981",
                    "desc": "厳密な最近傍ではなく高い確率で近い点を高速に見つける手法群。LSH・HNSW・IVF等がある。大規模ベクトルDBやレコメンドシステムで不可欠な技術"
                  },
                  {
                    "name": "LSH",
                    "color": "#10b981",
                    "desc": "Locality-Sensitive Hashing。類似データが同じバケットにハッシュされる確率が高い特殊なハッシュ関数を用いる高速近似最近傍探索法。サブリニア時間の検索を実現する"
                  },
                  {
                    "name": "HNSW",
                    "color": "#10b981",
                    "desc": "Hierarchical Navigable Small World。階層的なスモールワールドグラフ上で貪欲探索を行う近似最近傍探索手法。検索精度と速度の両方で優れ、現在最も広く使用される手法の一つ"
                  }
                ]
              }
            ]
          },
          {
            "name": "モデル評価・改善",
            "color": "#10b981",
            "desc": "学習したモデルの汎化性能を正しく評価し、過学習や過少適合を防いで性能を向上させるための手法群。評価指標・検証手法・正則化・アンサンブル・HP探索を包含する",
            "children": [
              {
                "name": "性能指標",
                "color": "#10b981",
                "desc": "分類モデルの性能を多角的に評価する指標群。正解率・適合率・再現率・F値・AUC等があり、タスクの特性(クラス不均衡・誤分類コストの非対称性等)に応じて適切な指標を選択する",
                "children": [
                  {
                    "name": "正解率",
                    "color": "#10b981",
                    "desc": "全サンプル中の正解予測の割合Accuracy=(TP+TN)/(TP+TN+FP+FN)。最も直感的な指標だがクラス不均衡データでは多数クラスに偏った予測でも高い値を示す欠点がある"
                  },
                  {
                    "name": "適合率",
                    "color": "#10b981",
                    "desc": "陽性と予測したサンプル中の真の陽性の割合Precision=TP/(TP+FP)。偽陽性のコストが高いタスク(スパム検出等)で重要。再現率とトレードオフの関係にある"
                  },
                  {
                    "name": "再現率",
                    "color": "#10b981",
                    "desc": "実際の陽性サンプル中のモデルが検出した割合Recall=TP/(TP+FN)。見逃し(偽陰性)のコストが高いタスク(医療診断・不良品検出等)で特に重視される指標"
                  },
                  {
                    "name": "F値",
                    "color": "#10b981",
                    "desc": "適合率Pと再現率Rの調和平均F₁=2PR/(P+R)。両指標のバランスを1つの値で評価する。β=2ならF₂(再現率重視)、β=0.5ならF₀.₅(適合率重視)に調整可能"
                  },
                  {
                    "name": "AUC",
                    "color": "#10b981",
                    "desc": "ROC曲線下の面積(Area Under Curve)。分類閾値に依存しないモデル全体の判別性能を0〜1で評価する指標。0.5がランダム、1.0が完全分類に対応する"
                  },
                  {
                    "name": "混同行列",
                    "color": "#10b981",
                    "desc": "予測ラベルと正解ラベルのクロス集計表。TP(真陽性)・FP(偽陽性)・TN(真陰性)・FN(偽陰性)の4セルで分類結果を可視化し、各種評価指標の算出基盤となる"
                  },
                  {
                    "name": "真陽性率",
                    "color": "#10b981",
                    "desc": "TPR=TP/(TP+FN)。実際の陽性のうち正しく陽性と予測した割合。感度(Sensitivity)・再現率(Recall)と同値。ROC曲線の縦軸に使用される重要な指標"
                  },
                  {
                    "name": "真陰性率",
                    "color": "#10b981",
                    "desc": "TNR=TN/(TN+FP)。実際の陰性のうち正しく陰性と予測した割合。特異度(Specificity)とも呼ばれ、健常者を正しく識別する能力を測る医学検査で重要な指標"
                  },
                  {
                    "name": "偽陽性率",
                    "color": "#10b981",
                    "desc": "FPR=FP/(FP+TN)。実際の陰性のうち誤って陽性と予測した割合。ROC曲線の横軸に使用される。1-特異度に等しく、誤報率(False Alarm Rate)とも呼ばれる"
                  },
                  {
                    "name": "偽陰性率",
                    "color": "#10b981",
                    "desc": "FNR=FN/(FN+TP)。実際の陽性のうち誤って陰性と予測した割合。見逃し率(Miss Rate)とも呼ばれ、1-再現率に等しい。安全性が重要なシステムで特に注意すべき指標"
                  },
                  {
                    "name": "決定係数",
                    "color": "#10b981",
                    "desc": "R²=1-SS_res/SS_tot。回帰モデルがデータの分散をどれだけ説明できるかを0〜1で表す指標。1に近いほど高い説明力を持ち、0は平均値予測と同等の性能を意味する"
                  },
                  {
                    "name": "マイクロ平均",
                    "color": "#10b981",
                    "desc": "全クラスのTP・FP・FNを合算してから指標を計算する平均方法。サンプル数の多いクラスの影響が大きくなる特性があり、全体的な正解率に近い値を示す"
                  },
                  {
                    "name": "マクロ平均",
                    "color": "#10b981",
                    "desc": "各クラスで独立に指標を算出した後、クラス数で単純平均する方法。少数クラスも多数クラスと等しく重みづけされるため、クラス不均衡データでの公平な評価に適する"
                  },
                  {
                    "name": "ROC曲線",
                    "color": "#10b981",
                    "desc": "分類閾値を0から1まで変化させたときの真陽性率(TPR)と偽陽性率(FPR)のプロット。左上に近いほど高性能。曲線下面積AUCでモデルの閾値非依存の性能を要約する"
                  },
                  {
                    "name": "PR曲線",
                    "color": "#10b981",
                    "desc": "分類閾値を変化させたときの適合率(縦軸)と再現率(横軸)のプロット。クラス不均衡データではROC曲線より性能差を鋭敏に反映するため、物体検出等で標準的に使用される"
                  },
                  {
                    "name": "AP",
                    "color": "#10b981",
                    "desc": "Average Precision。PR曲線下の面積として計算されるクラスごとの検出性能指標。IoU閾値ごとに算出され、物体検出タスクの標準的な単クラス評価指標"
                  },
                  {
                    "name": "mAP",
                    "color": "#10b981",
                    "desc": "Mean Average Precision。全クラスのAPの算術平均。物体検出・情報検索の総合評価指標として最も広く使用される。COCO評価ではIoU閾値0.5:0.95の平均mAPを用いる"
                  }
                ]
              },
              {
                "name": "誤差指標",
                "color": "#10b981",
                "desc": "回帰モデルの予測精度を定量化する指標群。MSE・RMSE・MAEが代表的で、外れ値への感度・解釈のしやすさ・最適化との相性等の観点から使い分ける",
                "children": [
                  {
                    "name": "平均二乗誤差",
                    "color": "#10b981",
                    "desc": "Σ(yᵢ-ŷᵢ)²/nで計算される回帰の基本損失関数。微分が容易で最適化に適するが、二乗により外れ値の影響を強く受ける。単位が元のデータの二乗になる欠点がある"
                  },
                  {
                    "name": "二乗平均平方根誤差",
                    "color": "#10b981",
                    "desc": "RMSE=√MSE。MSEの平方根を取ることで元のデータと同じ単位で解釈可能な誤差指標となる。平均的な予測誤差の大きさを直感的に示し、実務での報告に好まれる"
                  },
                  {
                    "name": "平均絶対誤差",
                    "color": "#10b981",
                    "desc": "Σ|yᵢ-ŷᵢ|/nで計算される誤差指標。外れ値に対してMSEより頑健で、中央値への回帰と関連する。ただし微分がゼロ点で不連続なため最適化にやや不向き"
                  },
                  {
                    "name": "MSE",
                    "color": "#10b981",
                    "desc": "Mean Squared Error。Σ(y-ŷ)²/nで計算される回帰の標準損失関数。勾配計算が容易で凸関数のため大域最適解が保証される。ガウスノイズ仮定下のMLEと等価"
                  },
                  {
                    "name": "RMSE",
                    "color": "#10b981",
                    "desc": "Root Mean Squared Error。√MSEで計算。元データと同じ単位で誤差を評価でき実用的な解釈性を持つ。MSEと同じ最適解を持つが、大きな誤差をより強く罰する特性がある"
                  },
                  {
                    "name": "MAE",
                    "color": "#10b981",
                    "desc": "Mean Absolute Error。Σ|y-ŷ|/nで計算。ラプラスノイズ仮定下のMLEに対応する。中央値推定量と関連し、外れ値に対してMSEより頑健な損失関数として使用される"
                  }
                ]
              },
              {
                "name": "検証手法",
                "color": "#10b981",
                "desc": "モデルの汎化性能(未知データへの予測力)を正しく推定するためのデータ分割・評価手法群。ホールドアウト法とk-分割交差検証法が代表的で、過学習の検出に不可欠",
                "children": [
                  {
                    "name": "k-分割交差検証法",
                    "color": "#10b981",
                    "desc": "データをk個(通常5〜10)に均等分割し、k-1個で訓練・1個で検証をk回ローテーションして平均性能を評価する手法。全データを検証に使えるため小規模データセットで特に有効"
                  },
                  {
                    "name": "ホールドアウト法",
                    "color": "#10b981",
                    "desc": "データを訓練・検証・テストに一度だけ分割する最も単純な検証法。大規模データで十分な評価が可能だが、分割の仕方によって評価のばらつきが大きい欠点がある"
                  },
                  {
                    "name": "訓練データ",
                    "color": "#10b981",
                    "desc": "モデルのパラメータ(重み・バイアス)の学習に直接使用するデータセット。通常は全データの60〜80%を割り当てる。このデータへの適合度が訓練誤差として計測される"
                  },
                  {
                    "name": "検証データ",
                    "color": "#10b981",
                    "desc": "ハイパーパラメータ調整・モデル選択・早期終了の判定に使用するデータセット。訓練中に定期的に評価し、過学習の検出と最適な学習停止時点の決定に用いる"
                  },
                  {
                    "name": "テストデータ",
                    "color": "#10b981",
                    "desc": "最終的なモデル性能の評価に使用するデータセット。モデル選択・調整が全て完了した後に一度だけ使用すべきで、このデータの性能が汎化性能の不偏推定量となる"
                  },
                  {
                    "name": "訓練誤差",
                    "color": "#10b981",
                    "desc": "訓練データに対するモデルの損失値。学習が進むにつれ減少するが、検証誤差との差(汎化ギャップ)が大きいと過学習を示唆する。モデル複雑度の増加で単調に減少する傾向"
                  },
                  {
                    "name": "汎化誤差",
                    "color": "#10b981",
                    "desc": "未知の全データに対するモデルの期待損失。直接計算は不可能でテストデータの誤差で近似する。バイアスとバリアンスの和に分解でき、この分解がモデル選択の理論的基盤"
                  },
                  {
                    "name": "学習データ",
                    "color": "#10b981",
                    "desc": "モデルの学習に用いるデータの総称。文脈により訓練データと同義で使われる場合と、訓練+検証データ全体を指す場合がある。適切なデータの量と質が学習性能を決定する"
                  }
                ]
              },
              {
                "name": "バイアス-バリアンス",
                "color": "#10b981",
                "desc": "汎化誤差をバイアス²+バリアンス+ノイズに分解する理論的枠組み。モデル複雑度の選択指針を与え、過少適合(高バイアス)と過学習(高バリアンス)のトレードオフを説明する",
                "children": [
                  {
                    "name": "バイアス",
                    "color": "#10b981",
                    "desc": "モデルの予測値の期待値と真の値との系統的なずれ。モデルが単純すぎる(容量不足)場合に大きくなり、データのパターンを十分に表現できない過少適合の原因となる"
                  },
                  {
                    "name": "バリアンス",
                    "color": "#10b981",
                    "desc": "異なる訓練データに対するモデルの予測値のばらつき。モデルが複雑すぎる場合に大きくなり、訓練データの個々のノイズに過敏に反応する過学習の原因となる"
                  },
                  {
                    "name": "過少適合",
                    "color": "#10b981",
                    "desc": "モデルが単純すぎてデータの真のパターンを捉えられない状態。訓練誤差も検証誤差も高い。モデル容量の増加・特徴量追加・正則化の緩和等で対処する"
                  },
                  {
                    "name": "過剰適合",
                    "color": "#10b981",
                    "desc": "モデルが訓練データに過度に適合し未知データで性能が低下する状態。訓練誤差は低いが検証誤差が高い。正則化・データ拡張・ドロップアウト・早期終了等で対処する"
                  }
                ]
              },
              {
                "name": "アンサンブル手法",
                "color": "#10b981",
                "desc": "複数の学習器を組み合わせて単一モデルより高い汎化性能を達成する手法群。バギング(分散低減)・ブースティング(バイアス低減)・スタッキング(異種統合)が三大アプローチ",
                "children": [
                  {
                    "name": "バギング",
                    "color": "#10b981",
                    "desc": "ブートストラップサンプル(復元抽出)で複数モデルを独立に学習し、予測を平均/多数決で統合する手法。個々のモデルの分散を低減し、ランダムフォレストの基盤となる"
                  },
                  {
                    "name": "ブースティング",
                    "color": "#10b981",
                    "desc": "前段のモデルが間違えたサンプルに重みを増やして次のモデルが学習する逐次的な手法。AdaBoost・XGBoost・LightGBMが代表で、バイアスの低減に効果的"
                  },
                  {
                    "name": "スタッキング",
                    "color": "#10b981",
                    "desc": "複数の異なるモデル(基本学習器)の予測をメタ学習器の入力として使い、最終予測を行う2段階のアンサンブル手法。異種モデルの強みを統合でき、コンペで頻用される"
                  },
                  {
                    "name": "ランダムフォレスト",
                    "color": "#10b981",
                    "desc": "バギング+各分岐での特徴量ランダム選択を組み合わせた決定木のアンサンブル。個々の木の多様性を高めることで高精度かつ過学習しにくいモデルを実現する代表的手法"
                  }
                ]
              },
              {
                "name": "ハイパーパラメータ探索",
                "color": "#10b981",
                "desc": "学習率・正則化係数・層数等の学習では決まらない設定値の最適な組合せを見つける過程。グリッドサーチ・ランダムサーチ・ベイズ最適化等の探索戦略がある",
                "children": [
                  {
                    "name": "グリッドサーチ",
                    "color": "#10b981",
                    "desc": "各ハイパーパラメータの候補値を格子状に設定し全組合せを網羅的に試す探索法。再現性が高く確実だが、パラメータ数が増えると組合せ爆発により計算量が指数的に増大する"
                  },
                  {
                    "name": "ランダムサーチ",
                    "color": "#10b981",
                    "desc": "ハイパーパラメータ空間からランダムにサンプリングして評価する探索法。グリッドサーチと同じ計算予算で、重要でないパラメータの無駄な探索を回避でき高次元で効率的"
                  },
                  {
                    "name": "ベイズ最適化",
                    "color": "#10b981",
                    "desc": "ガウス過程等の代理モデルと獲得関数を用いてブラックボックス関数を少ない評価回数で効率的に最適化する手法。NNのハイパーパラメータ探索で特に有効な手法として知られる"
                  },
                  {
                    "name": "獲得関数",
                    "color": "#10b981",
                    "desc": "ベイズ最適化で次に評価すべき点を決定する関数。EI(期待改善量)・UCB(上側信頼限界)・PI(改善確率)等があり、探索(未知領域)と活用(高性能領域)のバランスを制御する"
                  }
                ]
              },
              {
                "name": "正則化",
                "color": "#10b981",
                "desc": "モデルの複雑さにペナルティを課して過学習を防ぐ手法群。パラメータのノルムを制約するL1(スパース化)・L2(縮小化)正則化やWeight Decayを含む機械学習の基本技術",
                "children": [
                  {
                    "name": "L1正則化",
                    "color": "#10b981",
                    "desc": "損失関数にλΣ|wᵢ|を加えるペナルティ項。勾配が定数のためパラメータを厳密に0にでき、スパースな解を生む。不要な特徴量の自動選択効果があり高次元データで有効"
                  },
                  {
                    "name": "L2正則化",
                    "color": "#10b981",
                    "desc": "損失関数にλΣwᵢ²を加えるペナルティ項。パラメータを小さく保つことで過学習を抑制する。ガウス事前分布のMAP推定と等価で、Ridge回帰の正則化項に対応する"
                  },
                  {
                    "name": "Elastic Net",
                    "color": "#10b981",
                    "desc": "L1正則化(α)とL2正則化(1-α)を比率αで組み合わせた手法。Lassoのスパース性とRidgeの安定性を両立する。相関の高い特徴量グループを同時に選択する効果がある"
                  },
                  {
                    "name": "Weight Decay",
                    "color": "#10b981",
                    "desc": "各更新ステップで重みをw←(1-λη)wと一定割合で減衰させる正則化技法。SGDではL2正則化と等価だが、AdamWでは勾配適応とは独立に減衰させる分離型が効果的"
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "name": "深層学習\n（基礎）",
        "color": "#3b82f6",
        "desc": "NNの基礎理論と主要アーキテクチャ",
        "children": [
          {
            "name": "順伝播型NN",
            "color": "#3b82f6",
            "desc": "入力層→隠れ層→出力層へデータを一方向に伝播させる基本的なNN構造。多層パーセプトロン(MLP)が代表で、活性化関数による非線形変換と誤差逆伝播による学習が核心",
            "children": [
              {
                "name": "活性化関数",
                "color": "#3b82f6",
                "desc": "ニューラルネットワークの各層に非線形性を導入する関数。ReLU・Sigmoid・tanh・GELU・Softmax等があり、関数の選択がモデルの表現力・学習速度・勾配の安定性に大きく影響する",
                "children": [
                  {
                    "name": "ReLU",
                    "color": "#3b82f6",
                    "desc": "f(x)=max(0,x)。計算が高速で勾配消失を大幅に緩和する活性化関数。深層学習の事実上の標準。x<0で勾配が0になるdying neuron問題があるが、実用上は高い性能を示す"
                  },
                  {
                    "name": "Leaky ReLU",
                    "color": "#3b82f6",
                    "desc": "f(x)=max(αx,x)（α≈0.01）。ReLUのx<0領域に小さな傾きを与え、dying neuron問題を緩和する改良版。パラメトリック版PReLUではαも学習対象とする"
                  },
                  {
                    "name": "GELU",
                    "color": "#3b82f6",
                    "desc": "f(x)=x·Φ(x)（Φはガウス分布のCDF）。入力値の大きさに応じて滑らかにゲーティングする活性化関数。BERT・GPT・ViT等のTransformer系モデルの標準活性化関数"
                  },
                  {
                    "name": "Sigmoid function",
                    "color": "#3b82f6",
                    "desc": "σ(x)=1/(1+e^(-x))。出力を(0,1)に写像する関数。二値分類の出力層やLSTMのゲート機構で使用。飽和領域で勾配がほぼ0になる勾配消失の原因となりうる"
                  },
                  {
                    "name": "tanh",
                    "color": "#3b82f6",
                    "desc": "f(x)=(e^x-e^(-x))/(e^x+e^(-x))。出力を(-1,1)に写像。ゼロ中心なのでSigmoidより学習が安定する傾向がある。LSTMのセル状態やRNNの隠れ状態更新で使用される"
                  },
                  {
                    "name": "双曲線正接関数",
                    "color": "#3b82f6",
                    "desc": "tanh関数の正式名称。双曲線関数の一つでy=tanh(x)=(e^x-e^(-x))/(e^x+e^(-x))。出力が[-1,1]のゼロ中心であり、Sigmoid関数より学習効率が良い場合が多い"
                  },
                  {
                    "name": "Softmax",
                    "color": "#3b82f6",
                    "desc": "K次元ベクトルを確率分布に変換する関数。softmax(zᵢ)=exp(zᵢ)/Σexp(zⱼ)で全要素の和が1になる。多クラス分類の出力層で使用し、交差エントロピー損失と組み合わせる"
                  },
                  {
                    "name": "ソフトマックス関数",
                    "color": "#3b82f6",
                    "desc": "Softmaxの日本語表記。温度パラメータτでsoftmax(zᵢ/τ)とすることで出力の鋭さを調整可能。τ→0で最大値のone-hotに、τ→∞で一様分布に近づく"
                  }
                ]
              },
              {
                "name": "誤差逆伝播法",
                "color": "#3b82f6",
                "desc": "出力層の損失関数の値から各層のパラメータ勾配を連鎖律により逆方向に効率的に計算する学習アルゴリズム。1986年のRumelhart等の論文で広まり深層学習を可能にした核心技術",
                "children": [
                  {
                    "name": "連鎖律",
                    "color": "#3b82f6",
                    "desc": "合成関数y=f(g(x))の微分をdy/dx=(dy/dg)·(dg/dx)で計算する法則。多層NNにおける誤差逆伝播法の数学的基盤であり、各層の局所勾配の積で全体の勾配を求める"
                  },
                  {
                    "name": "計算グラフ",
                    "color": "#3b82f6",
                    "desc": "数式の各演算をノード、データの流れをエッジで表現した有向グラフ。順方向でデータを伝播し逆方向で勾配を伝播させる自動微分の実装基盤。PyTorch等で動的に構築される"
                  },
                  {
                    "name": "局所勾配",
                    "color": "#3b82f6",
                    "desc": "計算グラフの各ノード(演算)における入力に対する出力の偏微分値。逆伝播では上流から伝播してきた勾配にこの局所勾配を乗じて下流に伝える。各演算の微分規則に基づく"
                  },
                  {
                    "name": "自動微分",
                    "color": "#3b82f6",
                    "desc": "計算グラフを記録・追跡してプログラムの微分を自動的に計算するフレームワーク機能。手動での勾配導出が不要になり、複雑なモデルでも正確な勾配を効率的に計算できる"
                  },
                  {
                    "name": "back propagation",
                    "color": "#3b82f6",
                    "desc": "誤差逆伝播法の英語名。出力から入力へ向かって連鎖律に基づき勾配を逆方向に伝播させるアルゴリズム。計算量が順伝播と同程度のO(n)で効率的な勾配計算を実現する"
                  }
                ]
              },
              {
                "name": "損失関数",
                "color": "#3b82f6",
                "desc": "モデルの予測と正解の乖離を定量化する目的関数。回帰ではMSE・MAE、分類では交差エントロピー、セグメンテーションではDice損失等、タスクに応じて適切な関数を選択する",
                "children": [
                  {
                    "name": "MSE",
                    "color": "#3b82f6",
                    "desc": "Mean Squared Error。Σ(y-ŷ)²/nで計算される回帰の標準損失関数。勾配計算が容易で凸関数のため大域最適解が保証される。ガウスノイズ仮定下のMLEと等価"
                  },
                  {
                    "name": "クロスエントロピー誤差",
                    "color": "#3b82f6",
                    "desc": "真の分布pとモデル出力qの交差エントロピーH(p,q)=-Σp(x)log q(x)。分類問題の標準損失関数で、正解クラスの予測確率が高いほど損失が小さくなる性質を持つ"
                  },
                  {
                    "name": "バイナリクロスエントロピー誤差",
                    "color": "#3b82f6",
                    "desc": "二値分類の損失関数L=-[y·log(p)+(1-y)·log(1-p)]。正解y∈{0,1}に対する予測確率pの対数損失。マルチラベル分類でも各ラベル独立に適用される"
                  },
                  {
                    "name": "Focal Loss",
                    "color": "#3b82f6",
                    "desc": "交差エントロピーに(1-p_t)^γの重みを付けて簡単なサンプルの寄与を低減する損失関数。γ≥1で難しいサンプルに学習を集中させ、物体検出等のクラス不均衡問題に効果的"
                  },
                  {
                    "name": "Multi-task loss",
                    "color": "#3b82f6",
                    "desc": "複数タスクの損失を重み付き合計L=Σwᵢ·Lᵢで統合する損失関数。重みの設定にタスク不確実性を利用する手法(Uncertainty Weighting)等がある。マルチタスク学習の核心"
                  },
                  {
                    "name": "Dice係数",
                    "color": "#3b82f6",
                    "desc": "2つの集合の重なり度合いDice=2|A∩B|/(|A|+|B|)。セグメンテーション評価の標準指標で、1-Diceを損失関数として使用。クラス不均衡に強く医療画像解析で広く採用"
                  },
                  {
                    "name": "Jaccard係数",
                    "color": "#3b82f6",
                    "desc": "2つの集合の類似度J=|A∩B|/|A∪B|。IoU(Intersection over Union)と同値。Dice係数と単調な関係Dice=2J/(1+J)にあり、セグメンテーション評価で併用される"
                  }
                ]
              },
              {
                "name": "勾配消失・爆発",
                "color": "#3b82f6",
                "desc": "深層NNの学習で層数の増加に伴い勾配が指数的に減衰(消失)または増大(爆発)する問題。ReLU活性化・残差接続・勾配クリッピング・適切な重み初期化で対処する",
                "children": [
                  {
                    "name": "勾配消失",
                    "color": "#3b82f6",
                    "desc": "逆伝播で層を遡るごとに勾配が指数的に小さくなり、浅い層のパラメータがほぼ更新されなくなる問題。Sigmoid/tanhの飽和領域が主因。ReLUやResNetにより大幅に緩和された"
                  },
                  {
                    "name": "勾配消失問題",
                    "color": "#3b82f6",
                    "desc": "深層NNでSigmoid/tanh活性化関数の導関数が最大0.25/1.0であるため、多層にわたり勾配が乗算されるうちに指数的に減衰する現象。深層化の主要な障壁だったが現在は概ね解決"
                  },
                  {
                    "name": "勾配クリッピング",
                    "color": "#3b82f6",
                    "desc": "勾配のL2ノルムが閾値θを超えた場合にg←g·θ/||g||でスケーリングする手法。勾配爆発を防止しRNN/LSTMの学習で必須のテクニック。閾値の設定は経験的に決定"
                  },
                  {
                    "name": "Pathological Curvature",
                    "color": "#3b82f6",
                    "desc": "損失曲面の曲率(ヘシアン行列の固有値)が方向により極端に異なる病的な状態。急峻な方向と平坦な方向が共存し、SGDの収束を著しく遅くする。Adamなどの適応的手法で緩和"
                  }
                ]
              },
              {
                "name": "多層パーセプトロン",
                "color": "#3b82f6",
                "desc": "入力層・1つ以上の隠れ層・出力層からなる全結合ニューラルネットワーク(MLP)。万能近似定理により任意の連続関数を近似可能。全てのNNアーキテクチャの基礎となる構造"
              },
              {
                "name": "Affine層",
                "color": "#3b82f6",
                "desc": "入力xに対しy=Wx+bの線形(アフィン)変換を行う層。全結合層の線形演算部分を指し、活性化関数を適用する前の段階。行列積によるバッチ処理でGPU並列計算に適する"
              },
              {
                "name": "万能近似定理",
                "color": "#3b82f6",
                "desc": "十分な幅(ニューロン数)を持つ1隠れ層のNNは任意の連続関数をコンパクト集合上で任意の精度で近似できるという定理。NNの理論的表現力を保証するが必要な幅は指数的になりうる"
              },
              {
                "name": "全結合層",
                "color": "#3b82f6",
                "desc": "前層の全ニューロンが次層の全ニューロンに接続される層。Dense層とも呼ぶ。パラメータ数が入力×出力次元になるため、高次元入力では計算・メモリコストが大きくなる"
              }
            ]
          },
          {
            "name": "最適化",
            "color": "#3b82f6",
            "desc": "損失関数を最小化するパラメータを見つけるアルゴリズム群。SGDからAdam等の適応的手法まで多様な最適化器があり、学習率スケジューリングと重み初期化と合わせて学習の成否を左右する",
            "children": [
              {
                "name": "GD / SGD",
                "color": "#3b82f6",
                "desc": "勾配降下法のバリエーション群。全データの勾配(バッチGD)・1サンプルの勾配(SGD)・ミニバッチの勾配(ミニバッチSGD)で更新する。学習率とバッチサイズが主要なHP",
                "children": [
                  {
                    "name": "最急降下法",
                    "color": "#3b82f6",
                    "desc": "全訓練データの勾配∇L(θ)で更新θ←θ-η·∇L(θ)するバッチ勾配降下法。勾配推定が正確で安定だが、大規模データでは1回の更新に全データの処理が必要で計算コストが高い"
                  },
                  {
                    "name": "確率的勾配降下法",
                    "color": "#3b82f6",
                    "desc": "1サンプルの勾配で逐次更新するSGD。勾配推定にノイズがあるが計算が軽く、ノイズが局所最適解からの脱出を助ける。現代のDLでは通常ミニバッチ版を指す"
                  },
                  {
                    "name": "ミニバッチ勾配降下法",
                    "color": "#3b82f6",
                    "desc": "m個(通常32〜512)のサンプルで勾配を計算して更新する実用的な標準手法。バッチGDの安定性とSGDの計算効率を両立し、GPUの並列計算能力を効果的に活用できる"
                  },
                  {
                    "name": "バッチサイズ",
                    "color": "#3b82f6",
                    "desc": "1回のパラメータ更新で使用するサンプル数。大バッチは勾配推定が安定しGPU効率が良いが汎化性能が低下する傾向。小バッチはノイズが正則化効果を持つが学習が不安定になりうる"
                  },
                  {
                    "name": "学習率",
                    "color": "#3b82f6",
                    "desc": "パラメータ更新の步幅η。θ←θ-η·∇Lのηに対応。大きすぎると損失が発散し小さすぎると収束が遅い。コサイン減衰やウォームアップ等のスケジューリング戦略が重要"
                  }
                ]
              },
              {
                "name": "Momentum / NAG",
                "color": "#3b82f6",
                "desc": "過去の勾配情報(慣性)を蓄積して更新方向を安定化・加速する最適化手法群。Momentumが基本形でNAGが改良版。SGDの振動を抑え谷状の損失曲面での収束を高速化する",
                "children": [
                  {
                    "name": "Momentum",
                    "color": "#3b82f6",
                    "desc": "v←βv+∇L, θ←θ-ηvで過去の勾配方向を指数移動平均として蓄積する手法。β(通常0.9)の慣性により一貫した方向の更新が加速され、直交方向の振動が抑制される"
                  },
                  {
                    "name": "モメンタム",
                    "color": "#3b82f6",
                    "desc": "Momentum(慣性)の日本語表記。勾配の指数移動平均v_t=βv_{t-1}+(1-β)g_tを更新方向として使用し、損失曲面の谷に沿った効率的な最適化を実現する手法"
                  },
                  {
                    "name": "NAG",
                    "color": "#3b82f6",
                    "desc": "Nesterov Accelerated Gradient。θ-ηβvの「先読み位置」の勾配で修正する手法。v←βv+∇L(θ-ηβv)。Momentumの行き過ぎを事前に修正し、より速い収束を実現する"
                  },
                  {
                    "name": "Nesterov",
                    "color": "#3b82f6",
                    "desc": "Nesterov Accelerated Gradient法の提唱者名でもあり手法名。凸最適化で理論的に最適な収束率O(1/t²)を達成する加速勾配法。深層学習でもMomentumの改良として広く使用"
                  },
                  {
                    "name": "慣性",
                    "color": "#3b82f6",
                    "desc": "過去の更新方向を維持する性質。Momentum系手法で勾配ベクトルの指数移動平均として実装される。ランダムなノイズを平滑化し一貫した最適化方向を維持する効果がある"
                  }
                ]
              },
              {
                "name": "AdaGrad / RMSProp",
                "color": "#3b82f6",
                "desc": "各パラメータの過去勾配情報に基づき学習率を適応的に調整する手法群。頻出特徴の学習率は下げ、稀な特徴の学習率は維持する。Adamの構成要素として重要",
                "children": [
                  {
                    "name": "AdaGrad",
                    "color": "#3b82f6",
                    "desc": "各パラメータの過去勾配の二乗和Gを蓄積しθ←θ-η/√G·gで更新。スパースな特徴に大きな学習率を割り当てるが、Gが単調増加するため学習率が過度に減衰する欠点がある"
                  },
                  {
                    "name": "RMSProp",
                    "color": "#3b82f6",
                    "desc": "AdaGradの学習率減衰問題を指数移動平均v←ρv+(1-ρ)g²で緩和した手法。過去の勾配情報を徐々に忘却するため非定常環境に適応できる。Hintonが講義で提案した未公刊の手法"
                  },
                  {
                    "name": "パラメータ別学習率",
                    "color": "#3b82f6",
                    "desc": "各パラメータwᵢに対して個別の実効学習率η/√(vᵢ+ε)を適用する手法の総称。AdaGrad/RMSProp/Adamの核心的アイデアで、パラメータごとの勾配スケールの違いに対応する"
                  }
                ]
              },
              {
                "name": "Adam",
                "color": "#3b82f6",
                "desc": "Momentum(一次モーメント)とRMSProp(二次モーメント)を統合した適応的学習率最適化器。バイアス補正付きで実装が容易、ほぼ全てのDLタスクでデフォルトの選択肢となっている",
                "children": [
                  {
                    "name": "AdamW",
                    "color": "#3b82f6",
                    "desc": "Adamの重み減衰をL2正則化項から分離(decoupled weight decay)した改良版。Loshchilovら2019年提案。Transformer系モデルの標準オプティマイザとして広く採用されている"
                  },
                  {
                    "name": "一次モーメント",
                    "color": "#3b82f6",
                    "desc": "勾配の指数移動平均m_t=β₁m_{t-1}+(1-β₁)g_t。Momentumに相当し更新方向を安定化する。Adamではβ₁=0.9が標準で、勾配の平均的な方向を追跡する"
                  },
                  {
                    "name": "二次モーメント",
                    "color": "#3b82f6",
                    "desc": "勾配の二乗の指数移動平均v_t=β₂v_{t-1}+(1-β₂)g_t²。RMSPropに相当しパラメータ別学習率を実現する。Adamではβ₂=0.999が標準で、勾配の分散を追跡する"
                  },
                  {
                    "name": "バイアス補正",
                    "color": "#3b82f6",
                    "desc": "Adamの初期ステップでモーメント推定値がゼロに偏る問題を修正する処理。m̂_t=m_t/(1-β₁^t)、v̂_t=v_t/(1-β₂^t)として初期バイアスを除去し学習初期の安定性を向上"
                  }
                ]
              },
              {
                "name": "重み初期化",
                "color": "#3b82f6",
                "desc": "NNのパラメータの初期値設定法。適切な初期化により勾配消失・爆発を防ぎ学習を安定化する。Xavier法(Sigmoid/tanh向け)とHe法(ReLU向け)が代表的な手法",
                "children": [
                  {
                    "name": "Xavier法",
                    "color": "#3b82f6",
                    "desc": "Var(w)=2/(n_in+n_out)で重みを初期化する方法。入出力の分散を揃えることで信号の大きさを層間で保つ。Sigmoid・tanh等の対称な活性化関数と組合せて使用する"
                  },
                  {
                    "name": "He法",
                    "color": "#3b82f6",
                    "desc": "Var(w)=2/n_inで重みを初期化するReLU向け手法。ReLUが入力の半分(負側)をゼロにする効果を補正し、深層でも信号の分散が維持されるよう設計されている"
                  },
                  {
                    "name": "分散のスケーリング",
                    "color": "#3b82f6",
                    "desc": "各層の入出力ユニット数に応じて重みの分散を適切に設定する原理。順伝播時の信号と逆伝播時の勾配の分散が層を通じて一定に保たれるよう設計し学習の安定性を確保する"
                  }
                ]
              }
            ]
          },
          {
            "name": "正則化手法",
            "color": "#3b82f6",
            "desc": "深層NNの過学習を防止するためのテクニック群。ドロップアウト・正規化層・データ拡張・早期終了・L1/L2正則化等を含み、モデルの汎化性能向上に不可欠な技術体系",
            "children": [
              {
                "name": "ドロップアウト",
                "color": "#3b82f6",
                "desc": "訓練時に各ニューロンを確率pでランダムに無効化(出力を0に)する正則化手法。アンサンブル効果で過学習を抑制する。推論時は全ニューロンを使い出力をp倍にスケーリング",
                "children": [
                  {
                    "name": "ドロップコネクト",
                    "color": "#3b82f6",
                    "desc": "ニューロン単位ではなく結合(重み)をランダムに0にする正則化手法。ドロップアウトの一般化であり、理論的にはより細かい粒度での正則化が可能だが実装コストがやや高い"
                  },
                  {
                    "name": "ドロップ率p",
                    "color": "#3b82f6",
                    "desc": "各ニューロンを無効化する確率。全結合層ではp=0.5、畳み込み層ではp=0.2程度が一般的。推論時は全ニューロンを使用し重みを(1-p)倍するか訓練時に1/(1-p)倍する"
                  }
                ]
              },
              {
                "name": "正規化層",
                "color": "#3b82f6",
                "desc": "層の出力を統計量(平均・分散)で正規化して学習を安定化・高速化する技術。BN(バッチ)・LN(レイヤー)・GN(グループ)・IN(インスタンス)の4種がありアーキテクチャに応じて選択",
                "children": [
                  {
                    "name": "バッチ正規化",
                    "color": "#3b82f6",
                    "desc": "ミニバッチ内の同一チャネルで平均0・分散1に正規化後、学習可能なγ,βでスケール・シフトする手法。内部共変量シフトを緩和しCNNの学習を劇的に安定化・高速化する"
                  },
                  {
                    "name": "レイヤー正規化",
                    "color": "#3b82f6",
                    "desc": "各サンプルの全特徴量にわたって正規化する手法。バッチサイズに依存しないためRNN・Transformerの標準。特にTransformerのMulti-Head Attention前後に適用される"
                  },
                  {
                    "name": "グループ正規化",
                    "color": "#3b82f6",
                    "desc": "チャネルをG個のグループに分けてグループ内で正規化する手法。バッチサイズが小さい場合にBNより安定で、物体検出・セグメンテーション等のGPUメモリ制約下で有効"
                  },
                  {
                    "name": "インスタンス正規化",
                    "color": "#3b82f6",
                    "desc": "各サンプルの各チャネルで独立に正規化する手法(G=Cのグループ正規化と等価)。スタイル変換などの画像生成タスクでスタイル情報の除去に効果的"
                  }
                ]
              },
              {
                "name": "データ拡張",
                "color": "#3b82f6",
                "desc": "訓練データに回転・反転・色調変更等の変換を適用して人工的にデータ量を増やす手法。モデルが変換に対して不変な表現を学習し、実質的な正則化効果で汎化性能が大幅に向上する",
                "children": [
                  {
                    "name": "Flipping",
                    "color": "#3b82f6",
                    "desc": "画像の左右反転(水平フリップ)を行う最も基本的なデータ拡張。物体の向きが意味を持たないタスクで広く使用。上下反転は衛星画像等の特殊なケースで使用される"
                  },
                  {
                    "name": "Rotate",
                    "color": "#3b82f6",
                    "desc": "画像をランダムな角度(±10〜30°等)で回転させるデータ拡張。物体が任意の角度で出現しうるタスクで回転不変性を学習させる。補間方法と空白領域の処理が実装上のポイント"
                  },
                  {
                    "name": "Erase",
                    "color": "#3b82f6",
                    "desc": "Random Erasing。画像のランダムな矩形領域をランダム値またはゼロで置換するデータ拡張。物体の一部が遮蔽される状況への頑健性を向上させ、Cutoutと類似の効果がある"
                  },
                  {
                    "name": "Brightness",
                    "color": "#3b82f6",
                    "desc": "画像の輝度をランダムに変動させるデータ拡張。照明条件の変化に対するモデルの頑健性を向上させる。HSV色空間のV値を操作するか、RGB全体にオフセットを加える方法がある"
                  },
                  {
                    "name": "Contrast",
                    "color": "#3b82f6",
                    "desc": "画像のコントラスト(明暗の差)をランダムに変動させるデータ拡張。撮影条件やモニター特性の違いに対する頑健性を向上させる。画素値を平均値に向けて/から離して調整する"
                  },
                  {
                    "name": "明度",
                    "color": "#3b82f6",
                    "desc": "画像の明るさ(輝度)を調整する処理。HSV色空間のV(Value)チャネルを操作することが多い。環境光の変動をシミュレートするデータ拡張として使用される"
                  },
                  {
                    "name": "GaussianFilter",
                    "color": "#3b82f6",
                    "desc": "画像にガウシアンカーネルによるぼかしを適用するデータ拡張。カメラのフォーカスずれやモーションブラーをシミュレートし、微細なテクスチャへの過度な依存を防ぐ効果がある"
                  },
                  {
                    "name": "MixUp",
                    "color": "#3b82f6",
                    "desc": "2つの訓練サンプル(x₁,y₁),(x₂,y₂)をλx₁+(1-λ)x₂, λy₁+(1-λ)y₂で線形補間する拡張手法。決定境界を滑らかにし過学習を抑制。λはBeta分布からサンプルする"
                  },
                  {
                    "name": "RandAugment",
                    "color": "#3b82f6",
                    "desc": "N個の拡張操作をランダムに選び強度Mで適用する統一的自動拡張手法。AutoAugmentの探索コスト(GPU時間)を劇的に削減し、N=2,M=9程度のシンプルな設定で同等性能を実現"
                  },
                  {
                    "name": "EDA",
                    "color": "#3b82f6",
                    "desc": "Easy Data Augmentation。テキストに対する4つの単純な操作(同義語置換・ランダム挿入・ランダム入替・ランダム削除)を行う自然言語処理向けデータ拡張手法"
                  }
                ]
              },
              {
                "name": "早期終了",
                "color": "#3b82f6",
                "desc": "訓練中に検証データの損失を監視し改善が見られなくなった時点で学習を打ち切る正則化手法。過学習が進行する前に学習を止めることでモデルの暗黙的な複雑さを制限する効果がある",
                "children": [
                  {
                    "name": "patience",
                    "color": "#3b82f6",
                    "desc": "検証損失の改善がなくても学習を継続するエポック数。この回数を超えて改善がなければ学習を終了する。値が小さいと早すぎる停止、大きいと過学習の進行を許容してしまう"
                  },
                  {
                    "name": "検証損失",
                    "color": "#3b82f6",
                    "desc": "検証データに対する損失関数の値。訓練中にエポック毎に計算し、この値が上昇に転じた時点を過学習の開始と判断する。早期終了やモデル選択の判断基準として最も重要な監視指標"
                  }
                ]
              },
              {
                "name": "正則化",
                "color": "#3b82f6",
                "desc": "モデルの複雑さにペナルティを課して過学習を防ぐ手法群。パラメータのノルムを制約するL1(スパース化)・L2(縮小化)正則化やWeight Decayを含む機械学習の基本技術",
                "children": [
                  {
                    "name": "L1正則化",
                    "color": "#3b82f6",
                    "desc": "損失関数にλΣ|wᵢ|を加えるペナルティ項。勾配が定数のためパラメータを厳密に0にでき、スパースな解を生む。不要な特徴量の自動選択効果があり高次元データで有効"
                  },
                  {
                    "name": "L2正則化",
                    "color": "#3b82f6",
                    "desc": "損失関数にλΣwᵢ²を加えるペナルティ項。パラメータを小さく保つことで過学習を抑制する。ガウス事前分布のMAP推定と等価で、Ridge回帰の正則化項に対応する"
                  },
                  {
                    "name": "Weight Decay",
                    "color": "#3b82f6",
                    "desc": "各更新ステップで重みをw←(1-λη)wと一定割合で減衰させる正則化技法。SGDではL2正則化と等価だが、AdamWでは勾配適応とは独立に減衰させる分離型が効果的"
                  }
                ]
              }
            ]
          },
          {
            "name": "CNN",
            "color": "#3b82f6",
            "desc": "畳み込みニューラルネットワーク。局所的な畳み込み演算による特徴抽出とプーリングによる空間圧縮を階層的に繰り返す構造。画像認識を革新し計算機視覚の基盤アーキテクチャとなった",
            "children": [
              {
                "name": "畳み込み層",
                "color": "#3b82f6",
                "desc": "学習可能なフィルタ(カーネル)を入力にスライドさせて特徴マップを生成する層。局所受容野・重み共有・空間的階層により、画像の局所パターンを効率的に抽出する",
                "children": [
                  {
                    "name": "パディング",
                    "color": "#3b82f6",
                    "desc": "畳み込み前に入力の周囲にゼロ(zero-padding)等を追加して出力サイズを制御する処理。sameパディングで入出力サイズを維持、validパディング(なし)で縮小する。エッジ情報の保持にも寄与"
                  },
                  {
                    "name": "depth-wise畳み込み",
                    "color": "#3b82f6",
                    "desc": "各入力チャネルに独立に1つのフィルタを適用する軽量な畳み込み。通常の畳み込みの計算量の1/C_out倍に削減。MobileNet等の軽量モデルの核心技術"
                  },
                  {
                    "name": "point-wise畳み込み",
                    "color": "#3b82f6",
                    "desc": "1×1のカーネルサイズでチャネル方向の線形結合を行う畳み込み。depth-wiseと組合せてDepthwise Separable Convolutionを構成し計算量を大幅に削減する"
                  },
                  {
                    "name": "Group-wise Convolution",
                    "color": "#3b82f6",
                    "desc": "チャネルをG個のグループに分割し各グループ内で独立に畳み込む手法。計算量を1/Gに削減。ResNeXtで有効性が示されG=32で高い精度と効率のバランスを達成した"
                  },
                  {
                    "name": "グループ化畳み込み",
                    "color": "#3b82f6",
                    "desc": "Group-wise Convolutionの日本語表記。AlexNetで2GPU分割として偶然使用されたのが起源。意図的な設計としてはResNeXtが先駆で、モデルの多様性向上にも寄与する"
                  },
                  {
                    "name": "転置畳み込み",
                    "color": "#3b82f6",
                    "desc": "畳み込みの入出力サイズ関係を逆転させてアップサンプリングを行う層。フィルタも学習対象。セグメンテーション(FCN, U-Net)や生成モデル(DCGAN)のDecoder部で広く使用される"
                  },
                  {
                    "name": "逆畳み込み",
                    "color": "#3b82f6",
                    "desc": "転置畳み込み(Transposed Convolution)の俗称。厳密には畳み込みの逆演算ではなく、入力にゼロを挿入してから畳み込みを行う操作。deconvolutionとも呼ばれるが誤称"
                  }
                ]
              },
              {
                "name": "プーリング層",
                "color": "#3b82f6",
                "desc": "特徴マップの空間サイズを縮小する層。Max/Average/Global平均プーリング等がある。微小な位置変動に対する不変性を獲得し、上位層の受容野を拡大する効果がある",
                "children": [
                  {
                    "name": "プーリング",
                    "color": "#3b82f6",
                    "desc": "局所領域から代表値(最大値MaxPool/平均値AvgPool)を抽出して空間解像度を下げる操作。パラメータを持たず、位置不変性の獲得と計算量・メモリ削減に貢献する"
                  },
                  {
                    "name": "GAP",
                    "color": "#3b82f6",
                    "desc": "Global Average Pooling。特徴マップの全空間にわたり平均を取り1値にする層。全結合層の代わりに使用することでパラメータ数を劇的に削減し過学習を防ぐ。ResNet以降の標準構成"
                  },
                  {
                    "name": "Lpプーリング",
                    "color": "#3b82f6",
                    "desc": "プーリング領域内でLpノルムを計算する一般化プーリング。p=1で平均、p=2でL2(RMS)、p→∞でMaxプーリングに近づく。pを学習可能にして最適なプーリング方式を適応的に選択可能"
                  }
                ]
              },
              {
                "name": "代表モデル",
                "color": "#3b82f6",
                "desc": "CNN発展の系譜を代表する主要アーキテクチャ群。ResNetのスキップ接続が深層化の壁を突破し、ボトルネック構造とWideResNet等のバリエーションが実用性能を向上させた",
                "children": [
                  {
                    "name": "ResNet",
                    "color": "#3b82f6",
                    "desc": "残差ブロックF(x)+xによりスキップ接続を導入し100層以上の深層CNNの学習を可能にした革命的アーキテクチャ。2015年ILSVRC優勝。以降のほぼ全てのCNNの基盤となっている"
                  },
                  {
                    "name": "WideResNet",
                    "color": "#3b82f6",
                    "desc": "ResNetの深さを減らし幅(各層のチャネル数)を増やした変種。同じパラメータ数でより高い精度を達成し、GPU並列性も良い。深さより幅が重要であることを示した研究"
                  },
                  {
                    "name": "残差ブロック",
                    "color": "#3b82f6",
                    "desc": "y=F(x)+xの形で入力xを出力に直接加算するResNetの基本構成単位。Fが恒等写像からの差分(残差)のみを学習すればよいため最適化が容易になり深層化が可能になった"
                  },
                  {
                    "name": "スキップ接続",
                    "color": "#3b82f6",
                    "desc": "層を飛び越えて入力を出力に直接加算する接続。勾配が直接的に浅い層まで伝播できるため勾配消失を大幅に緩和する。ResNetで提案されU-Net・DenseNet等にも応用される"
                  },
                  {
                    "name": "ボトルネック構造",
                    "color": "#3b82f6",
                    "desc": "1×1(次元削減)→3×3(特徴抽出)→1×1(次元復元)の3層で構成するResNetの効率的残差ブロック。中間層のチャネル数を1/4に削減することで計算量を大幅に節約する"
                  }
                ]
              },
              {
                "name": "受容野",
                "color": "#3b82f6",
                "desc": "ある層のニューロンが入力画像のどの範囲の情報を受け取れるかの空間的領域。層が深くなるほど受容野は拡大する。物体検出ではアンカーサイズと受容野の整合が性能に影響する"
              },
              {
                "name": "単純型細胞",
                "color": "#3b82f6",
                "desc": "Hubel & Wieselが発見した視覚野の細胞。特定の方位・位置のエッジに選択的に反応する。CNNの畳み込み層が方位選択的なフィルタを学習することの生物学的な着想源"
              },
              {
                "name": "複雑型細胞",
                "color": "#3b82f6",
                "desc": "単純型細胞と異なり位置ずれに対して不変なエッジ応答を示す視覚野の細胞。CNNにおけるプーリング層の生物学的着想源であり、局所的な位置不変性の獲得に対応する"
              },
              {
                "name": "畳み込みニューラルネットワーク",
                "color": "#3b82f6",
                "desc": "CNNの正式な日本語名称。局所的受容野・重み共有・空間的階層構造の3原理に基づき画像の特徴を効率的に学習するアーキテクチャ。1989年のLeCunの研究が起源"
              }
            ]
          },
          {
            "name": "RNN",
            "color": "#3b82f6",
            "desc": "隠れ状態h_tを再帰的にh_t=f(h_{t-1},x_t)で更新して可変長の系列データを処理するNN。時系列予測・自然言語処理の基礎だが長期依存関係の学習が困難でLSTM/GRUで改良された",
            "children": [
              {
                "name": "LSTM",
                "color": "#3b82f6",
                "desc": "Long Short-Term Memory。メモリセルと忘却・入力・出力の3つのゲートにより長期記憶の保持と短期記憶の更新を制御するRNNアーキテクチャ。勾配消失問題を大幅に緩和した",
                "children": [
                  {
                    "name": "メモリセル",
                    "color": "#3b82f6",
                    "desc": "LSTMの核心となる内部状態C_t。ゲート機構により情報の追加(入力ゲート)・削除(忘却ゲート)・出力(出力ゲート)が制御され、必要な情報を長期間にわたって保持できる"
                  },
                  {
                    "name": "忘却ゲート",
                    "color": "#3b82f6",
                    "desc": "f_t=σ(W_f·[h_{t-1},x_t]+b_f)。メモリセルの過去の情報をどの程度保持するかを0〜1の値で制御する。1で完全保持、0で完全忘却。文脈の切り替え時に重要な役割を果たす"
                  },
                  {
                    "name": "入力ゲート",
                    "color": "#3b82f6",
                    "desc": "i_t=σ(W_i·[h_{t-1},x_t]+b_i)。新しい候補値C̃_tのうちどの程度をメモリセルに書き込むかを制御するゲート。重要な新情報の選択的な記憶を可能にする"
                  },
                  {
                    "name": "出力ゲート",
                    "color": "#3b82f6",
                    "desc": "o_t=σ(W_o·[h_{t-1},x_t]+b_o)。メモリセルの情報のうちどの程度を隠れ状態h_tとして出力するかを制御するゲート。h_t=o_t·tanh(C_t)で最終出力を生成する"
                  },
                  {
                    "name": "ゲート機構",
                    "color": "#3b82f6",
                    "desc": "シグモイド関数σで0〜1の制御信号を生成し、要素積で情報の流れを選択的に制御する仕組み。LSTM/GRUの核心であり、どの情報を保持・忘却・出力するかを学習的に決定する"
                  }
                ]
              },
              {
                "name": "GRU",
                "color": "#3b82f6",
                "desc": "Gated Recurrent Unit。LSTMの3ゲートを更新・リセットの2ゲートに簡略化した軽量なRNN。パラメータ数が少なく学習が速い。LSTMと同等の性能を示すことが多い",
                "children": [
                  {
                    "name": "更新ゲート",
                    "color": "#3b82f6",
                    "desc": "z_t=σ(W_z·[h_{t-1},x_t])。前の隠れ状態h_{t-1}をどの程度維持し、新しい候補状態h̃_tをどの程度反映するかを制御する。LSTMの忘却ゲートと入力ゲートの役割を統合"
                  },
                  {
                    "name": "リセットゲート",
                    "color": "#3b82f6",
                    "desc": "r_t=σ(W_r·[h_{t-1},x_t])。新しい候補状態h̃_tの計算時に前の隠れ状態h_{t-1}をどの程度無視するかを制御する。短期的な依存関係のリセットを可能にする"
                  }
                ]
              },
              {
                "name": "Seq2Seq",
                "color": "#3b82f6",
                "desc": "Encoder-Decoderモデルの別名。可変長の入力系列をEncoderで固定長ベクトルに圧縮し、Decoderで可変長の出力系列を生成する。機械翻訳・要約・対話で使用される",
                "children": [
                  {
                    "name": "Encoder-Decoder",
                    "color": "#3b82f6",
                    "desc": "入力系列をEncoder RNNで固定長のコンテキストベクトルに変換し、Decoder RNNでそこから出力系列を逐次生成するアーキテクチャ。Attention機構の追加で性能が大幅に向上した"
                  },
                  {
                    "name": "コンテキストベクトル",
                    "color": "#3b82f6",
                    "desc": "Encoderが入力系列全体の情報を要約した固定長の隠れ状態ベクトル。Decoderの初期状態として渡される。長い系列では情報のボトルネックになるためAttentionで改善された"
                  }
                ]
              },
              {
                "name": "BPTT法",
                "color": "#3b82f6",
                "desc": "Backpropagation Through Time。RNNを時間方向に展開した計算グラフ上で逆伝播を行うアルゴリズム。長い系列では計算コストと勾配消失が問題になりTruncated BPTTで対処する"
              },
              {
                "name": "双方向RNN",
                "color": "#3b82f6",
                "desc": "順方向(t=1→T)と逆方向(t=T→1)の2つのRNNの隠れ状態を結合するモデル。各時刻で過去と未来の両方のコンテキストを利用でき、BERTの双方向性の着想源となった"
              },
              {
                "name": "リカレントニューラルネットワーク",
                "color": "#3b82f6",
                "desc": "RNNの正式な日本語名称。隠れ状態の再帰的更新により系列データの時間的依存関係を捉える。Transformer登場以前は系列モデリングの主流アーキテクチャだった"
              }
            ]
          },
          {
            "name": "Transformer",
            "color": "#3b82f6",
            "desc": "Self-Attentionのみで系列を処理するアーキテクチャ。2017年Vaswaniら提案。RNNの再帰構造を排し全位置間の関係を並列計算可能にした。BERT・GPT・ViT等の基盤",
            "children": [
              {
                "name": "Self-Attention",
                "color": "#3b82f6",
                "desc": "入力系列の各要素がQuery・Key・Valueの3つの射影を持ち、全要素間の関連度(Attention重み)を計算して文脈依存の表現を生成する機構。Transformerの核心的演算",
                "children": [
                  {
                    "name": "Self-Attention機構",
                    "color": "#3b82f6",
                    "desc": "Attention(Q,K,V)=softmax(QK^T/√d_k)Vの計算。各位置が全位置の情報を重み付き集約して出力を生成する。計算量O(n²d)だが並列計算可能で長距離依存を直接捕捉する"
                  },
                  {
                    "name": "Query",
                    "color": "#3b82f6",
                    "desc": "注意の「問合せ」ベクトルQ=XW_Q。各位置が「どの情報に注目すべきか」を表す。Keyとの内積でAttentionスコアを算出し、関連度の高い位置のValueを強く集約する"
                  },
                  {
                    "name": "Key",
                    "color": "#3b82f6",
                    "desc": "注意の「キー」ベクトルK=XW_K。各位置が「どのような情報を提供できるか」を表す。Queryとの内積でAttentionスコアが計算されるため、QとKの整合が注意の質を決定する"
                  },
                  {
                    "name": "Value",
                    "color": "#3b82f6",
                    "desc": "注意の「値」ベクトルV=XW_V。Attentionスコアで重み付けされて出力を構成する実際の情報。QK^Tが「どこに注目するか」、Vが「何を取得するか」に対応する"
                  },
                  {
                    "name": "Scaled Dot-Product",
                    "color": "#3b82f6",
                    "desc": "Attention(Q,K,V)=softmax(QK^T/√d_k)V。内積をキー次元d_kの平方根で除算することでSoftmaxの入力の分散を1に保ち、勾配の安定化と学習効率の向上を図る"
                  }
                ]
              },
              {
                "name": "Multi-Head Attention",
                "color": "#3b82f6",
                "desc": "入力を複数のヘッド(通常8〜16個)に分割しそれぞれ独立にSelf-Attentionを計算後に結合する機構。異なる部分空間で異なる種類の関係性パターンを同時に捕捉できる",
                "children": [
                  {
                    "name": "Multi-Head Attention機構",
                    "color": "#3b82f6",
                    "desc": "MultiHead(Q,K,V)=Concat(head₁,...,headₕ)W_O、headᵢ=Attention(QWᵢQ,KWᵢK,VWᵢV)。h個のヘッドで多角的な表現を獲得し線形層で統合する"
                  },
                  {
                    "name": "ヘッド数",
                    "color": "#3b82f6",
                    "desc": "Multi-Head Attentionの並列ヘッド数h。d_model=512でh=8なら各ヘッドはd_k=64次元で計算。ヘッド数を増やすと多様な注意パターンを捕捉できるが計算量は同じ"
                  },
                  {
                    "name": "部分空間",
                    "color": "#3b82f6",
                    "desc": "Multi-Head Attentionの各ヘッドが担当するd_k=d_model/h次元の射影空間。各ヘッドが異なる部分空間で異なる種類の関係性(構文的・意味的・位置的)を学習する"
                  }
                ]
              },
              {
                "name": "Attention変種",
                "color": "#3b82f6",
                "desc": "基本的なGlobal Attentionに加え、Local(窓限定)・Masked(因果的マスク)・Source-Target(異系列間)の変種がある。タスクと計算制約に応じて適切な方式を選択する",
                "children": [
                  {
                    "name": "Global Attention機構",
                    "color": "#3b82f6",
                    "desc": "入力系列の全位置に対してAttentionを計算する標準的な方式。計算量O(n²)で全ての長距離依存を捕捉可能。長い系列ではSparse AttentionやLinear Attentionで効率化する"
                  },
                  {
                    "name": "Local Attention機構",
                    "color": "#3b82f6",
                    "desc": "注目位置の周辺w個の窓内でのみAttentionを計算する効率化方式。計算量O(nw)で長い系列にスケール可能。局所的依存が主要なタスクで効果的だが長距離依存の捕捉が制限される"
                  },
                  {
                    "name": "Masked Attention機構",
                    "color": "#3b82f6",
                    "desc": "未来の位置(t'>t)への注意をマスク(-∞)で遮断するAttention。Decoder側で自己回帰的な系列生成を行う際に必須。各位置は自分以前の位置の情報のみを参照できる"
                  },
                  {
                    "name": "Source Target Attention機構",
                    "color": "#3b82f6",
                    "desc": "EncoderとDecoder間で異なる系列間のAttentionを計算する機構。DecoderのQueryがEncoderのKey・Valueを参照し、入力系列の関連情報を出力生成に活用する"
                  }
                ]
              },
              {
                "name": "位置符号化",
                "color": "#3b82f6",
                "desc": "Transformerに系列の順序情報を付与する手法。Self-Attentionは位置に不変なため明示的な位置情報の注入が必要。正弦波ベースの固定式と学習ベースの2方式がある",
                "children": [
                  {
                    "name": "位置埋め込み",
                    "color": "#3b82f6",
                    "desc": "ViTでは各パッチの空間的位置情報を1D位置埋め込みとして付与する。Transformerが系列の順序情報を持たないため必須。2D位置埋め込みとの性能差は小さいことが報告されている"
                  },
                  {
                    "name": "正弦波",
                    "color": "#3b82f6",
                    "desc": "PE(pos,2i)=sin(pos/10000^(2i/d))の正弦波関数で位置を符号化する手法。学習不要で任意長の系列に対応可能。異なる周波数の正弦波が位置の相対関係を表現する"
                  },
                  {
                    "name": "学習可能PE",
                    "color": "#3b82f6",
                    "desc": "位置埋め込みベクトルを学習パラメータとして勾配降下で最適化する方式。BERT・GPT等で使用。データに適応した位置表現を獲得できるが、訓練時より長い系列への汎化が課題"
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "name": "深層学習\n（応用）",
        "color": "#8b5cf6",
        "desc": "画像・言語・生成・強化学習の応用",
        "children": [
          {
            "name": "画像認識",
            "color": "#8b5cf6",
            "desc": "画像に含まれる物体や場面のカテゴリを判別するタスク群。ViTによるTransformerベースの認識、転移学習による効率的な学習、Grad-CAM等による判断根拠の可視化を含む",
            "children": [
              {
                "name": "ViT",
                "color": "#8b5cf6",
                "desc": "Vision Transformer。画像を16×16等のパッチに分割しTransformerに入力する画像認識モデル。大規模データでCNNを凌駕する性能を示し画像認識のパラダイムを変革した",
                "children": [
                  {
                    "name": "パッチ分割",
                    "color": "#8b5cf6",
                    "desc": "画像をP×P(通常16×16)の固定サイズパッチに分割し各パッチを線形射影してトークン化するViTの入力処理。224×224画像で196個のトークンが生成されTransformerで処理される"
                  },
                  {
                    "name": "クラストークン",
                    "color": "#8b5cf6",
                    "desc": "[CLS]トークン。パッチトークン系列の先頭に追加される学習可能な特殊トークン。Self-Attention層を通過後、このトークンの出力が画像全体の表現として分類ヘッドに入力される"
                  },
                  {
                    "name": "位置埋め込み",
                    "color": "#8b5cf6",
                    "desc": "ViTでは各パッチの空間的位置情報を1D位置埋め込みとして付与する。Transformerが系列の順序情報を持たないため必須。2D位置埋め込みとの性能差は小さいことが報告されている"
                  }
                ]
              },
              {
                "name": "転移学習",
                "color": "#8b5cf6",
                "desc": "大規模データセット(ImageNet等)で事前学習したモデルの知識を対象タスクに再利用する手法。少量のデータでも高い性能を達成でき、学習時間とデータ収集コストを大幅に削減する",
                "children": [
                  {
                    "name": "ファインチューニング",
                    "color": "#8b5cf6",
                    "desc": "事前学習済みモデルの全体または最終数層のパラメータを対象タスクの小さなデータセットと低学習率で再学習する手法。特徴抽出器の微調整により高い転移性能を実現する"
                  },
                  {
                    "name": "事前学習",
                    "color": "#8b5cf6",
                    "desc": "大規模データで汎用的な特徴表現を学習する第1段階。ImageNetでの教師あり学習やMAE/DINO等の自己教師あり学習が主流。事前学習の質がファインチューニング後の性能を大きく左右する"
                  },
                  {
                    "name": "ドメイン",
                    "color": "#8b5cf6",
                    "desc": "データの統計的特性が共通する領域(例：自然画像・医療画像・衛星画像)。ドメイン間の分布差異がドメインシフトであり、転移学習ではソースとターゲットのドメイン選択が重要"
                  },
                  {
                    "name": "ドメインシフト",
                    "color": "#8b5cf6",
                    "desc": "訓練データ(ソースドメイン)とテストデータ(ターゲットドメイン)の間に統計的な分布の差異がある状況。撮影条件・センサー・地域差等が原因で、モデル性能低下の主因となる"
                  },
                  {
                    "name": "ドメイン適応",
                    "color": "#8b5cf6",
                    "desc": "ラベル付きソースドメインとラベルなしターゲットドメインの分布差を埋める技術。敵対的学習(DANN)やMMD最小化等で特徴空間のドメイン不変性を獲得する手法が代表的"
                  }
                ]
              },
              {
                "name": "CAM / Grad-CAM",
                "color": "#8b5cf6",
                "desc": "CNNがどの画像領域に注目して予測したかを可視化する説明可能AI(XAI)手法群。モデルの判断根拠を人間が理解可能な形で提示し、モデルの信頼性評価とデバッグに活用される",
                "children": [
                  {
                    "name": "CAM",
                    "color": "#8b5cf6",
                    "desc": "Class Activation Mapping。GAP直前の特徴マップを全結合層の重みで加重和して注目領域のヒートマップを生成する手法。GAPを持つCNNにのみ適用可能という制約がある"
                  },
                  {
                    "name": "Grad-CAM",
                    "color": "#8b5cf6",
                    "desc": "最終畳み込み層の特徴マップに対する出力クラスの勾配を重みとして使用し注目領域を可視化する手法。CAMと異なり任意のCNN構造に適用可能で、後方の全結合層を変更不要"
                  },
                  {
                    "name": "XAI",
                    "color": "#8b5cf6",
                    "desc": "Explainable AI(説明可能AI)。ブラックボックスなDLモデルの予測根拠を人間が理解できる形で提示する技術分野。Grad-CAM・SHAP・LIME等の手法があり医療・金融等で法的にも重要"
                  },
                  {
                    "name": "SHAP",
                    "color": "#8b5cf6",
                    "desc": "SHapley Additive exPlanations。ゲーム理論のShapley値に基づき各特徴量の予測への貢献度を公平に配分する説明手法。理論的に唯一の公理を満たす特徴量重要度の枠組み"
                  },
                  {
                    "name": "LIME",
                    "color": "#8b5cf6",
                    "desc": "Local Interpretable Model-agnostic Explanations。予測対象の入力周辺で局所的に線形モデルを近似して予測根拠を説明する手法。モデルの種類に依存しない汎用的な説明手法"
                  }
                ]
              }
            ]
          },
          {
            "name": "物体検出",
            "color": "#8b5cf6",
            "desc": "画像中の物体の位置(バウンディングボックス)とクラスを同時に予測するタスク。2段階(Faster R-CNN)と1段階(YOLO/SSD)の方式があり、自動運転・監視・医療等で広く応用",
            "children": [
              {
                "name": "2段階検出",
                "color": "#8b5cf6",
                "desc": "まず候補領域(Region Proposal)を生成し、次に各候補を分類・回帰する2ステップの物体検出手法。精度が高く、Faster R-CNNが代表。リアルタイム性は1段階手法に劣る傾向",
                "children": [
                  {
                    "name": "Faster R-CNN",
                    "color": "#8b5cf6",
                    "desc": "RPNとFast R-CNNを統合したEnd-to-Endの2段階物体検出器。2016年提案。RPNが候補領域を高速生成し、全体をGPU上で一括処理。高精度物体検出の基盤モデルとなった"
                  },
                  {
                    "name": "Selective Search",
                    "color": "#8b5cf6",
                    "desc": "色・テクスチャ・サイズの類似性で画像領域を階層的に統合し候補領域(約2000個)を生成する手法。R-CNN/Fast R-CNNで使用されたが、RPNの登場により置き換えられた"
                  },
                  {
                    "name": "RPN",
                    "color": "#8b5cf6",
                    "desc": "Region Proposal Network。特徴マップの各位置でk個のアンカーボックスに対し物体の有無と位置修正を予測する小型CNN。Faster R-CNNの候補領域生成を全てNN内で完結させた"
                  },
                  {
                    "name": "RoIプーリング",
                    "color": "#8b5cf6",
                    "desc": "可変サイズの候補領域(RoI)を固定サイズ(例7×7)の特徴マップに変換するプーリング。空間を均等分割しMax Poolingする。量子化(整数丸め)による位置ずれ誤差が存在する"
                  },
                  {
                    "name": "RoIAlign",
                    "color": "#8b5cf6",
                    "desc": "RoIプーリングの量子化誤差を双線形補間で解消した改良版。Mask R-CNNで導入。サブピクセル精度で特徴を抽出でき、特にセグメンテーションタスクで精度向上に大きく貢献した"
                  },
                  {
                    "name": "FPN",
                    "color": "#8b5cf6",
                    "desc": "Feature Pyramid Network。CNNの異なる深さの特徴マップをトップダウン経路とラテラル接続で統合し、マルチスケールの特徴ピラミッドを構築する。小さな物体の検出精度を大幅改善"
                  },
                  {
                    "name": "アンカーボックス",
                    "color": "#8b5cf6",
                    "desc": "各特徴マップ位置に配置される事前定義のアスペクト比・サイズの参照ボックス群。検出はアンカーからのオフセット回帰として定式化される。3×3=9種のアンカーが一般的"
                  }
                ]
              },
              {
                "name": "1段階検出",
                "color": "#8b5cf6",
                "desc": "候補領域生成を行わず特徴マップから直接物体のクラスと位置を予測する高速な検出手法。YOLO・SSD・RetinaNetが代表。リアルタイム処理に適するが微小物体の検出がやや弱い",
                "children": [
                  {
                    "name": "YOLO",
                    "color": "#8b5cf6",
                    "desc": "You Only Look Once。画像をグリッドに分割し各セルから直接バウンディングボックスとクラスを予測する1段階検出器。推論が非常に高速(>30FPS)でリアルタイム応用に適する"
                  },
                  {
                    "name": "SSD",
                    "color": "#8b5cf6",
                    "desc": "Single Shot MultiBox Detector。複数スケールの特徴マップそれぞれにアンカーを配置して直接検出する1段階手法。マルチスケール処理により小さな物体も検出可能にした"
                  },
                  {
                    "name": "ハードネガティブマイニング",
                    "color": "#8b5cf6",
                    "desc": "大量の負例(背景)の中から誤検出が多い難しい負例を優先的に学習に使用する手法。正例:負例=1:3等の比率で選択し、極端なクラス不均衡による学習の不安定化を防ぐ"
                  }
                ]
              },
              {
                "name": "アンカーフリー",
                "color": "#8b5cf6",
                "desc": "事前定義のアンカーボックスを使わず物体の中心点やコーナー等から直接検出する手法群。アンカーのHP(サイズ・比率・数)設計が不要になりモデル設計が大幅にシンプルになる",
                "children": [
                  {
                    "name": "FCOS",
                    "color": "#8b5cf6",
                    "desc": "Fully Convolutional One-Stage detector。各ピクセルから4方向のバウンディングボックス距離を直接回帰するアンカーフリー検出器。センターネスとFPNの組合せで高い性能を達成"
                  },
                  {
                    "name": "アンカーフリー手法",
                    "color": "#8b5cf6",
                    "desc": "アンカーボックスに依存しない物体検出手法の総称。FCOS・CenterNet・CornerNet等。ハイパーパラメータの削減とシンプルな設計が利点で、最近の検出研究の主流になりつつある"
                  },
                  {
                    "name": "センターネス",
                    "color": "#8b5cf6",
                    "desc": "各ピクセルが物体の中心からどれだけ近いかを[0,1]で表すスコア。FCOSで導入され、物体の周辺部からの低品質な予測を抑制して検出精度を向上させる。NMS前に予測スコアに乗算"
                  }
                ]
              },
              {
                "name": "Mask R-CNN",
                "color": "#8b5cf6",
                "desc": "Faster R-CNNにピクセル単位のマスク予測ブランチを追加した検出+インスタンスセグメンテーション統合モデル。RoIAlignの導入で高精度なマスク予測を実現した2017年の重要モデル",
                "children": [
                  {
                    "name": "マスクブランチ",
                    "color": "#8b5cf6",
                    "desc": "Mask R-CNNでRoIAlignの出力に対して28×28のバイナリマスクをクラスごとに予測するFCN分岐。分類・回帰・マスクの3タスクを並列に処理するマルチタスク学習構成"
                  }
                ]
              },
              {
                "name": "検出評価指標",
                "color": "#8b5cf6",
                "desc": "物体検出の性能を定量評価する指標群。予測と正解の重なりIoU、重複排除NMS、クラスごとの精度AP、総合指標mAPを組合せて多角的にモデルの検出能力を評価する",
                "children": [
                  {
                    "name": "IoU",
                    "color": "#8b5cf6",
                    "desc": "Intersection over Union=|A∩B|/|A∪B|。予測と正解のバウンディングボックスの重なり度合いを0〜1で測る指標。通常IoU≥0.5を正解判定の閾値として使用(COCO評価では0.5:0.95)"
                  },
                  {
                    "name": "NMS",
                    "color": "#8b5cf6",
                    "desc": "Non-Maximum Suppression。重複する検出ボックスから最高スコアのものだけを残す後処理。IoU閾値(通常0.5)以上重なるボックスを順次除去する。Soft-NMS等の改良版も存在する"
                  },
                  {
                    "name": "mAP",
                    "color": "#8b5cf6",
                    "desc": "Mean Average Precision。全クラスのAPの算術平均。物体検出・情報検索の総合評価指標として最も広く使用される。COCO評価ではIoU閾値0.5:0.95の平均mAPを用いる"
                  },
                  {
                    "name": "AP",
                    "color": "#8b5cf6",
                    "desc": "Average Precision。PR曲線下の面積として計算されるクラスごとの検出性能指標。IoU閾値ごとに算出され、物体検出タスクの標準的な単クラス評価指標"
                  }
                ]
              }
            ]
          },
          {
            "name": "セグメンテーション",
            "color": "#8b5cf6",
            "desc": "画像の各ピクセルにラベルを付与するタスク群。セマンティック(クラス別)・インスタンス(個体別)・パノプティック(統合)・バイナリ(2値)の4種に分類される精密な画像理解技術",
            "children": [
              {
                "name": "セマンティック",
                "color": "#8b5cf6",
                "desc": "画像の全ピクセルにクラスラベル(道路・建物・人等)を付与するセグメンテーション。同クラスの異なる個体は区別しない。FCN・DeepLab等のEncoder-Decoder構造が基本",
                "children": [
                  {
                    "name": "セマンティックセグメンテーション",
                    "color": "#8b5cf6",
                    "desc": "ピクセル単位で「何が」あるかをクラスラベルで分類するタスク。自動運転(道路・歩道認識)・医療画像(臓器領域)・衛星画像(土地利用分類)等で広く応用される"
                  },
                  {
                    "name": "FCN",
                    "color": "#8b5cf6",
                    "desc": "Fully Convolutional Network。CNNの全結合層を1×1畳み込みに置換しピクセル単位の予測を可能にしたセグメンテーションの先駆的モデル。2015年提案。任意サイズの入力に対応可能"
                  }
                ]
              },
              {
                "name": "インスタンス",
                "color": "#8b5cf6",
                "desc": "同クラスの異なる物体個体をピクセル単位で区別するセグメンテーション。物体検出+マスク予測のアプローチが主流で、Mask R-CNNが代表。「人1」「人2」のように個体を識別する",
                "children": [
                  {
                    "name": "インスタンスセグメンテーション",
                    "color": "#8b5cf6",
                    "desc": "画像中の各物体個体をピクセル単位のマスクで分離するタスク。物体検出(位置特定)とセマンティックセグメンテーション(ピクセル分類)を統合した高度な画像理解技術"
                  }
                ]
              },
              {
                "name": "パノプティック",
                "color": "#8b5cf6",
                "desc": "セマンティック(背景:空・道路等のstuff)とインスタンス(前景:車・人等のthing)のセグメンテーションを統合する包括的タスク。シーン全体の完全な理解を目指す",
                "children": [
                  {
                    "name": "パノプティックセグメンテーション",
                    "color": "#8b5cf6",
                    "desc": "画像の全ピクセルに対し背景クラス(stuff)はセマンティック的に、前景物体(things)はインスタンス的にラベル付けする統合タスク。PQ(Panoptic Quality)で評価される"
                  }
                ]
              },
              {
                "name": "バイナリ",
                "color": "#8b5cf6",
                "desc": "画像の各ピクセルを前景(関心領域)と背景の2クラスに分類する最もシンプルなセグメンテーション。医療画像の腫瘍領域抽出・衛星画像の建物抽出等が代表的な応用例",
                "children": [
                  {
                    "name": "バイナリセグメンテーション",
                    "color": "#8b5cf6",
                    "desc": "前景/背景の2クラスにピクセル分類するタスク。Dice損失やバイナリCE損失を使用。U-Net等のEncoder-Decoder構造にスキップ接続を組合せたモデルが標準的に使用される"
                  },
                  {
                    "name": "U-Net",
                    "color": "#8b5cf6",
                    "desc": "対称なEncoder-Decoder構造とスキップ接続を持つセグメンテーションモデル。2015年に医療画像向けに提案。少量の訓練データでも高精度な結果を得られ、広く様々な領域で応用されている"
                  },
                  {
                    "name": "スキップ接続",
                    "color": "#8b5cf6",
                    "desc": "層を飛び越えて入力を出力に直接加算する接続。勾配が直接的に浅い層まで伝播できるため勾配消失を大幅に緩和する。ResNetで提案されU-Net・DenseNet等にも応用される"
                  }
                ]
              }
            ]
          },
          {
            "name": "自然言語処理",
            "color": "#8b5cf6",
            "desc": "テキストデータの処理・理解・生成を行う技術分野。Word Embeddingによる分散表現、BERTによる双方向文脈理解、GPTによる自己回帰生成、対照学習による表現学習を扱う",
            "children": [
              {
                "name": "Word Embedding",
                "color": "#8b5cf6",
                "desc": "単語を固定長の密なベクトル(通常100〜300次元)に写像する手法。意味的に類似した単語が近いベクトルになり、king-man+woman≈queenのようなアナロジー関係も表現される",
                "children": [
                  {
                    "name": "CBOW",
                    "color": "#8b5cf6",
                    "desc": "Continuous Bag of Words。周囲のw個の単語(文脈窓)から中心単語を予測してWord2Vecを学習する方式。skip-gramより高速で頻出語に強い。入力の順序は無視する(Bag-of-Words)"
                  },
                  {
                    "name": "skip-gram",
                    "color": "#8b5cf6",
                    "desc": "中心単語から周囲の文脈単語を予測してWord2Vecを学習する方式。CBOWの逆方向。低頻度語の表現学習に強く、より大きなコーパスで効果を発揮する。計算コストはCBOWより高い"
                  },
                  {
                    "name": "ネガティブサンプリング",
                    "color": "#8b5cf6",
                    "desc": "全語彙でSoftmaxを計算する代わりにk個(通常5〜20)の負例単語のみをサンプルして対照的に学習する効率化手法。Word2Vecの学習を実用的な速度に高速化した鍵技術"
                  },
                  {
                    "name": "one-hotベクトル",
                    "color": "#8b5cf6",
                    "desc": "語彙V中のi番目の単語を第i成分のみ1で他が全て0のV次元スパースベクトルで表す表現。単語間の類似性を表現できない欠点があり、Embedding層で密なベクトルに変換される"
                  },
                  {
                    "name": "単語埋め込み",
                    "color": "#8b5cf6",
                    "desc": "Word Embeddingの日本語表現。各単語をd次元の実数ベクトルに写像するルックアップテーブル。単語の意味的類似性がベクトル空間のコサイン距離で測れる分散表現を生成する"
                  },
                  {
                    "name": "単語分散表現",
                    "color": "#8b5cf6",
                    "desc": "単語をd次元の実数ベクトルで表現する手法の総称。Word2Vec・GloVe・FastTextが代表。「分布仮説(類似した文脈で使われる単語は類似した意味を持つ)」が理論的基盤"
                  }
                ]
              },
              {
                "name": "BERT",
                "color": "#8b5cf6",
                "desc": "Bidirectional Encoder Representations from Transformers。MLMとNSPで事前学習するTransformer Encoderモデル。双方向文脈を捉えた汎用表現を獲得しNLPの多数のタスクでSOTA達成",
                "children": [
                  {
                    "name": "MLM",
                    "color": "#8b5cf6",
                    "desc": "Masked Language Model。入力トークンの15%をランダムにマスクし元のトークンを予測する事前学習タスク。双方向の文脈情報を活用した表現学習を実現するBERTの核心手法"
                  },
                  {
                    "name": "NSP",
                    "color": "#8b5cf6",
                    "desc": "Next Sentence Prediction。2つの文が実際に連続しているかを判定する二値分類の事前学習タスク。文間関係の理解に寄与するが、後続研究では効果が限定的とされ除外されることも多い"
                  }
                ]
              },
              {
                "name": "GPT",
                "color": "#8b5cf6",
                "desc": "Generative Pre-trained Transformer。左から右への自己回帰的に次の単語を予測するTransformer Decoderモデル。スケーリング則に従い巨大化するほど性能が向上する大規模言語モデルの基盤",
                "children": [
                  {
                    "name": "ゼロショット",
                    "color": "#8b5cf6",
                    "desc": "タスク固有の例示なしにモデルがタスクを遂行する能力。大規模言語モデルが事前学習で獲得した汎用知識のみで多様なタスクに対応できることを示し、GPT-3で広く実証された"
                  },
                  {
                    "name": "フューショット",
                    "color": "#8b5cf6",
                    "desc": "少数(2〜数十個)の入出力例をプロンプトに含めてタスクを解かせる手法。モデルのパラメータ更新なしに例示から入出力パターンを推論するIn-context learningの主要な形態"
                  },
                  {
                    "name": "ワンショット",
                    "color": "#8b5cf6",
                    "desc": "1つの例示のみでタスクを遂行させる手法。ゼロショットとフューショットの中間。最小限の情報からパターンを般化する能力を測る指標としても使用される"
                  },
                  {
                    "name": "プロンプト",
                    "color": "#8b5cf6",
                    "desc": "大規模言語モデルへの入力テキスト。指示(instruction)・文脈(context)・例示(examples)・質問(query)を含む構成で、モデルの出力品質はプロンプト設計に大きく依存する"
                  },
                  {
                    "name": "プロンプトベース学習",
                    "color": "#8b5cf6",
                    "desc": "モデルのパラメータ更新なしにプロンプト(入力テキスト)の工夫によってタスク性能を向上させる学習パラダイム。Prompt Engineering・Chain-of-Thought等の技法を含む"
                  }
                ]
              },
              {
                "name": "Contrastive学習",
                "color": "#8b5cf6",
                "desc": "正例ペア(類似)を近づけ負例ペア(非類似)を遠ざけるように表現空間を学習する手法。SimCLR・MoCo・CLIP等の自己教師あり学習で大きな成果を上げた表現学習の主要パラダイム",
                "children": [
                  {
                    "name": "Contrastive learning",
                    "color": "#8b5cf6",
                    "desc": "対照学習の英語表現。データ拡張で同一画像の2つのビューを正例ペア、異なる画像を負例ペアとして表現を学習する。大量のラベルなしデータから高品質な特徴表現を獲得できる"
                  },
                  {
                    "name": "Contrastive loss",
                    "color": "#8b5cf6",
                    "desc": "正例ペアの距離を縮め負例ペアの距離を広げる損失関数L=-log(exp(sim(z_i,z_j)/τ)/Σexp(sim(z_i,z_k)/τ))。InfoNCE損失とも呼ばれ、温度τが分布の鋭さを制御する"
                  },
                  {
                    "name": "Triplet loss",
                    "color": "#8b5cf6",
                    "desc": "アンカーa・正例p・負例nの三つ組でL=max(d(a,p)-d(a,n)+margin, 0)を最小化する損失。顔認識(FaceNet)で提案。ハードネガティブの選択が学習効率に大きく影響する"
                  },
                  {
                    "name": "Siamese network",
                    "color": "#8b5cf6",
                    "desc": "同一構造・同一パラメータの2つのNNで入力ペアを処理し出力の距離を比較するアーキテクチャ。顔照合・署名検証・ワンショット学習等のペア比較タスクで広く使用される"
                  },
                  {
                    "name": "Triplet network",
                    "color": "#8b5cf6",
                    "desc": "Siameseを3分岐に拡張しアンカー・正例・負例のトリプレットを同時処理するアーキテクチャ。Triplet lossで学習し、3つの入力間の相対的な距離関係を効率的に学習する"
                  },
                  {
                    "name": "コントラスト",
                    "color": "#8b5cf6",
                    "desc": "正例と負例のペアリングに基づいて表現空間の構造を学習する手法の日本語総称。自己教師あり学習の文脈で、データ拡張と組合せた対照学習(Contrastive learning)を指すことが多い"
                  }
                ]
              }
            ]
          },
          {
            "name": "生成モデル",
            "color": "#8b5cf6",
            "desc": "データの確率分布P(x)を学習し新しいサンプルを生成するモデル群。AE・VAE・GAN・フローベース・拡散モデルが5大アプローチ。画像生成・テキスト生成・創薬等に応用される",
            "children": [
              {
                "name": "オートエンコーダ",
                "color": "#8b5cf6",
                "desc": "入力xをEncoder→低次元表現z→Decoderで再構成x'する自己教師あり学習モデル。ボトルネックにより本質的な特徴表現を抽出。次元削減・異常検出・前処理に使用される",
                "children": [
                  {
                    "name": "デノイジングオートエンコーダ",
                    "color": "#8b5cf6",
                    "desc": "入力にノイズ(ガウスノイズ・マスキング等)を加えた破損データから元の入力を復元するAE。ノイズ除去の過程でより頑健で汎化性の高い特徴表現を獲得できる"
                  },
                  {
                    "name": "表現学習",
                    "color": "#8b5cf6",
                    "desc": "データの有用な内部表現(特徴量)を自動的に学習すること。手作業の特徴量設計を不要にする。AEのボトルネック表現、対照学習の埋め込み、BERTの文脈表現等が代表例"
                  },
                  {
                    "name": "ボトルネック",
                    "color": "#8b5cf6",
                    "desc": "AEの中間層の次元を入力より大幅に低く設定した構造。情報のボトルネックがデータの本質的な構造の抽出を強制する。情報理論のInformation Bottleneck原理と関連する"
                  }
                ]
              },
              {
                "name": "VAE",
                "color": "#8b5cf6",
                "desc": "Variational Autoencoder。潜在変数zの事後分布q(z|x)を近似し確率的な生成モデルを構築する手法。再構成誤差+KLダイバージェンスのELBOを最大化して学習する",
                "children": [
                  {
                    "name": "変分オートエンコーダ",
                    "color": "#8b5cf6",
                    "desc": "VAEの正式名称。Encoderが潜在変数の分布パラメータ(μ,σ)を出力し、Decoderがサンプルされたzからデータを再構成する。連続的で滑らかな潜在空間を学習する"
                  },
                  {
                    "name": "リパラメタリゼーション・トリック",
                    "color": "#8b5cf6",
                    "desc": "z=μ+σ·ε(ε~N(0,1))と表すことで確率的サンプリングを微分可能な決定的操作に変換する技法。VAEの勾配ベース学習を可能にした核心的なアイデア"
                  },
                  {
                    "name": "ELBO",
                    "color": "#8b5cf6",
                    "desc": "Evidence Lower Bound(変分下界)。対数周辺尤度log P(D)の下界で、ELBO=E[log P(D|z)]-D_KL(Q(z)||P(z))と分解される。VAEや変分推論の最適化目的関数として中心的な役割"
                  },
                  {
                    "name": "温度パラメータ",
                    "color": "#8b5cf6",
                    "desc": "Softmaxのスケーリング係数τ。softmax(z/τ)でτ→0は最大値のone-hot(決定的)、τ→∞は一様分布(ランダム)に近づく。生成の多様性と品質のトレードオフを制御する"
                  }
                ]
              },
              {
                "name": "GAN",
                "color": "#8b5cf6",
                "desc": "Generative Adversarial Network。生成器(Generator)と識別器(Discriminator)が敵対的に学習するゲーム理論的な生成モデル。高品質な画像生成を実現したが学習の不安定性が課題",
                "children": [
                  {
                    "name": "敵対的生成ネットワーク",
                    "color": "#8b5cf6",
                    "desc": "GANの正式名称。G(z)が生成した偽データとreal dataをDが判別し、GはDを騙す方向に学習するmin-maxゲーム。ナッシュ均衡でP_G=P_dataに収束する理論的保証がある"
                  },
                  {
                    "name": "Generator",
                    "color": "#8b5cf6",
                    "desc": "ランダムノイズz~P(z)から偽データG(z)を生成するNN。識別器を欺く方向にmin_G E[log(1-D(G(z)))]で学習する。画像生成ではConvTranspose層を用いたアーキテクチャが一般的"
                  },
                  {
                    "name": "Discriminator",
                    "color": "#8b5cf6",
                    "desc": "入力が本物(real)か偽物(fake)かを判別する二値分類NN。max_D E[log D(x)]+E[log(1-D(G(z)))]で学習。GeneratorのPGSの品質のフィードバックシグナルを提供する"
                  },
                  {
                    "name": "モード崩壊",
                    "color": "#8b5cf6",
                    "desc": "GANの生成器が多様性を失い訓練データの一部のモード(パターン)のみを出力する現象。分布全体を網羅的に学習できず同じような画像ばかり生成する。WGAN等で緩和される"
                  },
                  {
                    "name": "WGAN",
                    "color": "#8b5cf6",
                    "desc": "Wasserstein距離を損失関数に用いたGAN。JS距離の勾配消失問題を解消し学習を安定化。Discriminatorの代わりにCritic(リプシッツ制約付き)を使用する。WGAN-GPが実用的改良版"
                  },
                  {
                    "name": "Wasserstein距離",
                    "color": "#8b5cf6",
                    "desc": "最適輸送理論に基づく確率分布間の距離W(P,Q)=inf E[||x-y||]。分布の台が重ならない場合もJS距離と異なり有意な勾配を提供し、WGANの安定学習の理論的根拠となる"
                  },
                  {
                    "name": "DCGAN",
                    "color": "#8b5cf6",
                    "desc": "Deep Convolutional GAN。BN使用・全結合層排除・ReLU/LeakyReLU活性化等のCNNベースGANの設計指針を確立した2016年の重要論文。以降のGAN研究のアーキテクチャ基盤となった"
                  },
                  {
                    "name": "CGAN",
                    "color": "#8b5cf6",
                    "desc": "Conditional GAN。クラスラベルy等の条件情報をG・D双方に入力し条件付き生成P(x|y)を可能にしたGAN拡張。特定カテゴリの画像生成やimage-to-image変換(pix2pix)に応用"
                  }
                ]
              },
              {
                "name": "フローベース",
                "color": "#8b5cf6",
                "desc": "可逆変換f(x)=zの連鎖で単純な分布(ガウス)を複雑なデータ分布に変換する生成モデル。変数変換公式で正確な尤度計算が可能。RealNVP・Glow・NICE等が代表的手法",
                "children": [
                  {
                    "name": "フローベース生成モデル",
                    "color": "#8b5cf6",
                    "desc": "正規化フローの正式名称。可逆な変換f₁∘f₂∘...∘fₖにより基底分布p(z)からデータ分布p(x)を構成する。対数尤度を厳密に計算・最適化でき、潜在表現の解釈性も高い"
                  },
                  {
                    "name": "カップリングレイヤー",
                    "color": "#8b5cf6",
                    "desc": "入力を(x₁,x₂)に分割しx₂をx₁の関数で変換する可逆層。y₁=x₁, y₂=x₂⊙exp(s(x₁))+t(x₁)の形式(affine coupling)。逆変換が容易でヤコビアンが三角行列になる"
                  },
                  {
                    "name": "ActNorm",
                    "color": "#8b5cf6",
                    "desc": "Activation Normalization。データの1バッチ目の統計量で初期化するチャネル単位のアフィン変換。BNのバッチサイズ依存性を排除しGlow等のフローモデルで使用される正規化手法"
                  },
                  {
                    "name": "加法的分解",
                    "color": "#8b5cf6",
                    "desc": "カップリングレイヤーの基本形式。入力を2分割しy₁=x₁, y₂=x₂+t(x₁)で変換する加法的カップリング。逆変換はx₂=y₂-t(y₁)と容易。NICEで提案されたフローの原型"
                  }
                ]
              },
              {
                "name": "拡散モデル",
                "color": "#8b5cf6",
                "desc": "データに段階的にノイズを加える前方過程と、ノイズを除去する逆過程(NNで学習)で構成される生成モデル。DDPM・Score-based等。2020年以降にGANを超える画像生成品質を達成",
                "children": [
                  {
                    "name": "ノイズ",
                    "color": "#8b5cf6",
                    "desc": "拡散モデルの前方過程で各ステップtでデータに加えるガウスノイズε~N(0,I)。スケジュールβ_tで制御され、T→∞で純粋なガウスノイズに収束する。逆過程でε_θ(x_t,t)を予測して除去"
                  },
                  {
                    "name": "前方過程",
                    "color": "#8b5cf6",
                    "desc": "データx₀にスケジュールβ_tに従いT段階で漸進的にガウスノイズを加えq(x_t|x_{t-1})=N(√(1-β_t)x_{t-1},β_tI)でx_Tをほぼ純粋なノイズに変換する決定的な拡散過程"
                  },
                  {
                    "name": "逆過程",
                    "color": "#8b5cf6",
                    "desc": "純粋なノイズx_Tから段階的にデノイズしてデータx₀を復元する過程p_θ(x_{t-1}|x_t)。NNがノイズε_θまたはスコア∇log p(x_t)を予測して学習する。サンプリングの品質と速度が研究課題"
                  }
                ]
              },
              {
                "name": "識別モデル",
                "color": "#8b5cf6",
                "desc": "条件付き確率P(y|x)を直接モデル化して入力xからラベルyを予測する分類モデル。ロジスティック回帰・SVM・NN分類器等。生成モデルP(x,y)=P(x|y)P(y)と対比される概念"
              }
            ]
          },
          {
            "name": "深層強化学習",
            "color": "#8b5cf6",
            "desc": "NNを関数近似器として強化学習の価値関数や方策を表現する手法群。DQNのAtariゲーム超人的プレイやAlphaGo等で大きな成果。価値ベース・方策ベース・Actor-Criticに分類される",
            "children": [
              {
                "name": "価値ベース",
                "color": "#8b5cf6",
                "desc": "状態価値V(s)または行動価値Q(s,a)をNNで近似し、最大価値の行動を選択する方策を間接的に導出する手法群。Q学習・DQNが代表。離散的な行動空間のタスクに適する",
                "children": [
                  {
                    "name": "Q学習",
                    "color": "#8b5cf6",
                    "desc": "行動価値関数Q(s,a)をベルマン最適方程式Q(s,a)←r+γ·max_a'Q(s',a')で逐次更新するオフポリシーなRL手法。経験再生と組合せたDQNでNN関数近似が実用化された"
                  },
                  {
                    "name": "DQN",
                    "color": "#8b5cf6",
                    "desc": "Deep Q-Network。Q関数をNNで近似し、経験再生バッファとターゲットネットワーク(固定Q)の2つの技法で学習を安定化した手法。2015年にAtariゲームで人間超えの性能を実証"
                  },
                  {
                    "name": "行動価値関数",
                    "color": "#8b5cf6",
                    "desc": "状態sで行動aを取りその後最適方策に従った場合の期待累積報酬Q*(s,a)=E[Σγ^t·r_t|s₀=s,a₀=a]。Q学習ではこの関数の最適値を求め、argmax_a Q(s,a)で行動選択する"
                  },
                  {
                    "name": "TD学習",
                    "color": "#8b5cf6",
                    "desc": "Temporal Difference学習。次状態の推定価値を用いて現在の価値をδ=r+γV(s')-V(s)のTD誤差で逐次更新する手法。モンテカルロ法(完了まで待つ)とDP(モデル必要)の中間的手法"
                  }
                ]
              },
              {
                "name": "方策ベース",
                "color": "#8b5cf6",
                "desc": "方策π(a|s;θ)をNNでパラメータ化し期待報酬J(θ)=E[Σr_t]を勾配上昇法で直接最適化する手法群。連続行動空間に自然に適用でき、確率的方策の学習が可能",
                "children": [
                  {
                    "name": "方策勾配法",
                    "color": "#8b5cf6",
                    "desc": "∇J(θ)=E[Σ∇logπ(a_t|s_t;θ)·G_t]で方策パラメータの勾配を推定し更新する手法。REINFORCE算法が基本形。高いバリアンスが課題でベースラインの導入やActor-Criticで改善"
                  },
                  {
                    "name": "方策勾配定理",
                    "color": "#8b5cf6",
                    "desc": "期待報酬の勾配を∇J(θ)=E_π[∇logπ(a|s;θ)·Q^π(s,a)]と表せるという定理。方策の微分可能性のみを要求し報酬関数や状態遷移の微分は不要なためモデルフリーRLの基盤"
                  },
                  {
                    "name": "方策オン型",
                    "color": "#8b5cf6",
                    "desc": "現在の方策πで収集したデータのみで学習するRL。SARSA・A2C等。方策外のデータは使えずデータ効率は低いが、方策とデータの一致により学習が安定する傾向がある"
                  },
                  {
                    "name": "方策オフ型",
                    "color": "#8b5cf6",
                    "desc": "行動方策μ≠学習方策πのデータを再利用して学習するRL。Q学習・DQN・SAC等。経験再生により過去データを再利用でき、データ効率が高い。重要度サンプリングで補正する"
                  }
                ]
              },
              {
                "name": "Actor-Critic",
                "color": "#8b5cf6",
                "desc": "方策(Actor)と価値関数(Critic)を同時に学習する手法。方策勾配のバリアンスをCriticの価値推定で低減する。REINFORCE+ベースラインの自然な発展形で実用的な標準手法",
                "children": [
                  {
                    "name": "A3C",
                    "color": "#8b5cf6",
                    "desc": "Asynchronous Advantage Actor-Critic。複数のワーカーが異なる環境で非同期に経験を収集・学習する並列RL手法。経験再生バッファ不要で多様な経験を効率的に収集する"
                  },
                  {
                    "name": "Advantage",
                    "color": "#8b5cf6",
                    "desc": "A(s,a)=Q(s,a)-V(s)。特定の行動aの価値と状態の平均的価値の差。方策勾配でQ(s,a)の代わりにA(s,a)を使うとバリアンスが低減し学習が安定する。A2C/A3C/PPOで使用"
                  }
                ]
              }
            ]
          },
          {
            "name": "その他の学習",
            "color": "#8b5cf6",
            "desc": "距離学習・説明可能AI・基盤モデル等の重要な関連分野群。メトリック学習による類似度空間の構築、XAIによるモデル透明性の確保、大規模基盤モデルの活用を含む",
            "children": [
              {
                "name": "距離学習",
                "color": "#8b5cf6",
                "desc": "データ間の距離・類似度を学習するメトリックラーニング。Contrastive loss・Triplet lossでembedding空間を学習し、顔認識・画像検索・推薦等の類似度ベースのタスクに応用される"
              },
              {
                "name": "XAI(説明性)",
                "color": "#8b5cf6",
                "desc": "Explainable AI。ブラックボックスモデルの判断根拠を人間が理解可能な形で提示する技術。Grad-CAM(画像)・SHAP(特徴量)・LIME(局所近似)等。医療・金融・法律分野で重要性が増大"
              },
              {
                "name": "基盤モデル",
                "color": "#8b5cf6",
                "desc": "大規模データで事前学習した汎用的なAIモデル(Foundation Model)。BERT・GPT・CLIP・SAM等。下流タスクへの転移学習やプロンプトベースの活用で様々なタスクに適応可能",
                "children": [
                  {
                    "name": "ロバスト性",
                    "color": "#8b5cf6",
                    "desc": "入力のノイズ・摂動・分布変化・敵対的攻撃に対するモデルの頑健性。データ拡張・敵対的訓練・分布外検出等で向上させる。実運用環境での安全性と信頼性に直結する重要な性質"
                  },
                  {
                    "name": "NLP",
                    "color": "#8b5cf6",
                    "desc": "Natural Language Processing(自然言語処理)。テキストの理解・生成・翻訳・要約等を行う技術分野。基盤モデルの主要な応用領域であり、GPT・BERT等が革新的な成果を上げている"
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "name": "開発・運用\n環境",
        "color": "#ec4899",
        "desc": "フレームワーク・高速化・軽量化・仮想化",
        "children": [
          {
            "name": "DLフレームワーク",
            "color": "#ec4899",
            "desc": "深層学習モデルの実装・学習・推論を効率化するソフトウェアライブラリ。PyTorch(研究主流)・TensorFlow(産業主流)・NumPy(数値計算基盤)が三大ツール。自動微分が核心機能",
            "children": [
              {
                "name": "PyTorch",
                "color": "#ec4899",
                "desc": "Meta(旧Facebook)開発のDLフレームワーク。Define-by-Run(動的計算グラフ)方式でPythonの直感的な記述が可能。研究コミュニティで圧倒的なシェアを持つ現在の事実上の標準",
                "children": [
                  {
                    "name": "nn.Module",
                    "color": "#ec4899",
                    "desc": "PyTorchのモデル定義の基底クラス。__init__で層を定義しforward()で順伝播を記述する。パラメータ管理・GPU転送・保存/読込等の機能を提供するOOPベースの設計"
                  },
                  {
                    "name": "autograd",
                    "color": "#ec4899",
                    "desc": "PyTorchの自動微分エンジン。Tensorの演算を動的に計算グラフとして記録し、.backward()呼び出しで全パラメータの勾配を自動計算する。Define-by-Runの中核機能"
                  },
                  {
                    "name": "DataLoader",
                    "color": "#ec4899",
                    "desc": "PyTorchのデータ読み込みユーティリティ。Datasetからミニバッチの自動構成・シャッフル・num_workersによる並列読込を行う。学習ループの標準的なデータパイプライン"
                  }
                ]
              },
              {
                "name": "TensorFlow",
                "color": "#ec4899",
                "desc": "Google開発のDLフレームワーク。TF2.0以降はEager実行+tf.kerasが標準。TF Lite(モバイル)・TF.js(ブラウザ)・TF Serving(推論)等のエコシステムが充実し産業応用に強い",
                "children": [
                  {
                    "name": "tf.keras",
                    "color": "#ec4899",
                    "desc": "TensorFlowの高レベルAPI。Sequential(逐次積み上げ)・Functional(複雑な結合)・Subclassing(カスタム)の3方式でモデルを構築。fit/evaluate/predictの簡潔なインターフェース"
                  },
                  {
                    "name": "GradientTape",
                    "color": "#ec4899",
                    "desc": "TensorFlowのEager実行モードにおける自動微分API。with tf.GradientTape()ブロック内の演算を記録し、tape.gradient()で任意の変数に対する勾配を計算する"
                  }
                ]
              },
              {
                "name": "NumPy",
                "color": "#ec4899",
                "desc": "Pythonの数値計算基盤ライブラリ。N次元配列ndarrayとブロードキャスト演算を提供。C実装による高速計算。PyTorch/TensorFlowのテンソルAPIはNumPyに準拠して設計されている",
                "children": [
                  {
                    "name": "ndarray",
                    "color": "#ec4899",
                    "desc": "NumPyのN次元配列オブジェクト。dtype(データ型)・shape(形状)・stride(メモリ配置)を持ち、要素単位演算・スライシング・ブロードキャストを効率的に実行するデータ構造"
                  },
                  {
                    "name": "ブロードキャスト",
                    "color": "#ec4899",
                    "desc": "形状の異なる配列間で自動的に次元を拡張して演算を可能にするNumPy/PyTorch/TFの仕組み。(3,4)と(4,)の加算を(3,4)と(1,4)→(3,4)と解釈して計算する。ベクトル化計算の鍵"
                  }
                ]
              }
            ]
          },
          {
            "name": "ハードウェア",
            "color": "#ec4899",
            "desc": "DLの計算を実行するプロセッサ群。GPU(汎用並列計算)・TPU(Google専用ML ASIC)・CPU(汎用逐次処理)がある。行列演算の並列化が学習の高速化の鍵で、選択がコストと性能を左右する",
            "children": [
              {
                "name": "GPU / CUDA",
                "color": "#ec4899",
                "desc": "NVIDIA GPUとそのプログラミングプラットフォームCUDA。数千のコアによる大規模並列行列演算が可能で、DL学習をCPU比で10〜100倍高速化する。現在のDL研究・産業の標準ハードウェア",
                "children": [
                  {
                    "name": "GPU",
                    "color": "#ec4899",
                    "desc": "Graphics Processing Unit。元来はグラフィックス処理用だが数千コアの並列計算能力がDLの行列演算に適合。NVIDIA A100/H100等のデータセンター向けGPUがDL学習の標準計算基盤"
                  },
                  {
                    "name": "CUDA",
                    "color": "#ec4899",
                    "desc": "Compute Unified Device Architecture。NVIDIA GPUの汎用並列コンピューティングプラットフォーム。PyTorch/TFがバックエンドとして使用しGPU上のテンソル演算を実行する基盤技術"
                  },
                  {
                    "name": "cuDNN",
                    "color": "#ec4899",
                    "desc": "CUDA Deep Neural Network library。畳み込み・プーリング・正規化・活性化等のDL基本演算をGPU上で最適化した高速ライブラリ。PyTorch/TFが内部で自動的に呼び出す"
                  },
                  {
                    "name": "SIMT",
                    "color": "#ec4899",
                    "desc": "Single Instruction Multiple Threads。GPUのアーキテクチャ。多数のスレッド(Warp=32スレッド)が同一命令を並列実行する。条件分岐でスレッド間の処理が分岐するとwarp divergenceが発生"
                  }
                ]
              },
              {
                "name": "TPU / MXU",
                "color": "#ec4899",
                "desc": "GoogleのTensor Processing UnitとそのMatrix Multiply Unit。ML特化のASICチップで、特にTransformerの大規模学習で高い性能を発揮する。Google Cloud経由でアクセス可能",
                "children": [
                  {
                    "name": "TPU",
                    "color": "#ec4899",
                    "desc": "Tensor Processing Unit。Google独自開発のML特化ASIC。systolic arrayアーキテクチャのMXUで行列演算を高速処理。BFloat16データ型でメモリ効率と計算速度を両立する"
                  },
                  {
                    "name": "MXU",
                    "color": "#ec4899",
                    "desc": "Matrix Multiply Unit。TPU内部の行列積演算専用プロセッサ。128×128のsystolic arrayで大規模行列積をクロックサイクルごとにパイプライン処理し超高スループットを実現する"
                  }
                ]
              },
              {
                "name": "CPU / SIMD / MIMD",
                "color": "#ec4899",
                "desc": "汎用プロセッサCPUとその並列処理方式。SIMD(1命令で複数データ処理)とMIMD(複数命令を独立実行)。GPUの補完としてデータ前処理・軽量推論・制御フロー処理で使用される",
                "children": [
                  {
                    "name": "CPU",
                    "color": "#ec4899",
                    "desc": "Central Processing Unit。少数(4〜128)の高性能コアで逐次処理に優れる汎用プロセッサ。分岐予測・キャッシュ最適化で複雑な処理を高速実行。DLではデータ前処理と軽量推論に使用"
                  },
                  {
                    "name": "SIMD",
                    "color": "#ec4899",
                    "desc": "Single Instruction Multiple Data。1命令で複数のデータ要素を同時処理するCPU命令セット。AVX-512やNEON等。ベクトル演算の高速化に使用され、NumPy等の内部実装で活用される"
                  },
                  {
                    "name": "MIMD",
                    "color": "#ec4899",
                    "desc": "Multiple Instruction Multiple Data。複数のプロセッサが独立に異なる命令を実行する並列方式。マルチコアCPU・分散コンピューティングが該当。データ並列化のワーカー間の独立計算に対応"
                  }
                ]
              }
            ]
          },
          {
            "name": "高速化技術",
            "color": "#ec4899",
            "desc": "DLの学習・推論を高速化する技術群。複数GPUでデータを分割するデータ並列化とモデルを分割するモデル並列化がある。大規模モデルの学習時間を数日〜数週間から数時間に短縮する",
            "children": [
              {
                "name": "データ並列化",
                "color": "#ec4899",
                "desc": "同一モデルを複数のGPUにコピーし、異なるミニバッチを各GPUで並列処理して勾配を集約する手法。最も広く使用される分散学習方式。PyTorchのDistributedDataParallelが標準実装",
                "children": [
                  {
                    "name": "同期型",
                    "color": "#ec4899",
                    "desc": "全ワーカーのミニバッチ処理完了を待ち、AllReduceで勾配を集約してから一括更新する方式。単一GPU学習と数学的に等価な結果を保証するが、最も遅いワーカーに律速される欠点がある"
                  },
                  {
                    "name": "非同期型",
                    "color": "#ec4899",
                    "desc": "各ワーカーが独立にパラメータサーバーの重みを読み書きする方式。遅いワーカーを待たず高速だが、古い重みで計算した勾配(stale gradient)による学習の不安定性が課題"
                  },
                  {
                    "name": "AllReduce",
                    "color": "#ec4899",
                    "desc": "全ワーカーの勾配テンソルを効率的に集約(Reduce)し全ワーカーに配布(Broadcast)する通信アルゴリズム。Ring-AllReduceは帯域幅最適で、NCCLライブラリが高速実装を提供する"
                  }
                ]
              },
              {
                "name": "モデル並列化",
                "color": "#ec4899",
                "desc": "1つのGPUに載りきらない巨大モデルを複数GPUに分割して配置する手法。層単位の分割(パイプライン並列)とテンソル単位の分割(テンソル並列)がある。GPT等の超大規模モデルで必須",
                "children": [
                  {
                    "name": "分散深層学習",
                    "color": "#ec4899",
                    "desc": "複数のGPU/TPU/ノードに計算を分散してDLの学習を高速化する技術の総称。データ並列・モデル並列・パイプライン並列を組合せてスケーリングする。Horovod・DeepSpeed等のツールがある"
                  },
                  {
                    "name": "パイプライン並列",
                    "color": "#ec4899",
                    "desc": "モデルをステージ(複数層のグループ)に分割し各GPUに配置、ミニバッチをマイクロバッチに分けてパイプライン的に処理する手法。GPipe・PipeDreamが代表。バブル(空き時間)の最小化が課題"
                  }
                ]
              }
            ]
          },
          {
            "name": "軽量化技術",
            "color": "#ec4899",
            "desc": "学習済みモデルのサイズ・計算量を削減してエッジデバイスでの推論を可能にする技術群。量子化(ビット幅削減)・蒸留(知識転移)・プルーニング(パラメータ削除)の3大アプローチがある",
            "children": [
              {
                "name": "量子化",
                "color": "#ec4899",
                "desc": "モデルのパラメータや演算をFP32からINT8/INT4等の低ビット幅に変換してメモリと計算量を削減する技術。PTQ(後処理)とQAT(学習中)の2方式がある。推論速度を数倍に高速化可能",
                "children": [
                  {
                    "name": "PTQ",
                    "color": "#ec4899",
                    "desc": "Post Training Quantization。学習済みモデルに対し追加学習なしでキャリブレーションデータのみで量子化パラメータを決定する手法。手軽に適用可能だが精度低下がQATより大きい傾向"
                  },
                  {
                    "name": "QAT",
                    "color": "#ec4899",
                    "desc": "Quantization-Aware Training。学習中に量子化の影響(丸め誤差)をシミュレートしてモデルが量子化に適応するよう学習する手法。追加の学習コストがかかるがPTQより高い精度を維持する"
                  },
                  {
                    "name": "INT8",
                    "color": "#ec4899",
                    "desc": "8ビット整数での推論。FP32(32bit)からの変換で4倍のメモリ効率と2〜4倍の推論高速化が期待できる。TensorRT・ONNX Runtime等の推論エンジンがINT8最適化をサポートする"
                  }
                ]
              },
              {
                "name": "蒸留",
                "color": "#ec4899",
                "desc": "大きく高精度なTeacherモデルの知識を小さく軽量なStudentモデルに転移する技術。Teacherのソフトラベル(確率分布)がクラス間関係の暗黙知を含み、ハードラベルより豊かな教師信号を提供",
                "children": [
                  {
                    "name": "Teacher",
                    "color": "#ec4899",
                    "desc": "知識蒸留における教師モデル。高精度だが大きく計算コストが高い。Softmax出力の確率分布(ソフトラベル)に含まれるクラス間の類似性情報を生徒モデルに伝達する役割"
                  },
                  {
                    "name": "Student",
                    "color": "#ec4899",
                    "desc": "知識蒸留における生徒モデル。小さく軽量で推論高速。L_student=α·CE(y,p_s)+(1-α)·KL(p_t^τ||p_s^τ)のようにハードラベルとTeacherのソフトラベルの両方から学習する"
                  },
                  {
                    "name": "温度パラメータ",
                    "color": "#ec4899",
                    "desc": "Softmaxのスケーリング係数τ。softmax(z/τ)でτ→0は最大値のone-hot(決定的)、τ→∞は一様分布(ランダム)に近づく。生成の多様性と品質のトレードオフを制御する"
                  }
                ]
              },
              {
                "name": "プルーニング",
                "color": "#ec4899",
                "desc": "学習済みモデルの重要度の低いパラメータ・ニューロン・フィルタを除去してモデルを圧縮する技術。「宝くじ仮説」(重要な部分ネットワークが存在する)が理論的背景として注目されている",
                "children": [
                  {
                    "name": "構造化",
                    "color": "#ec4899",
                    "desc": "フィルタ・チャネル・層といった構造的単位で一括除去する構造化プルーニング。除去後のモデルは通常のテンソル演算で処理でき、専用ハードウェアなしで実際の推論高速化を実現できる"
                  },
                  {
                    "name": "非構造化",
                    "color": "#ec4899",
                    "desc": "個々のパラメータ(重み)単位で除去する非構造化プルーニング。90%以上の高い圧縮率が可能だが、スパースなテンソルの効率的処理には専用ハードウェアやライブラリが必要となる"
                  }
                ]
              }
            ]
          },
          {
            "name": "仮想化・環境構築",
            "color": "#ec4899",
            "desc": "DL開発環境の構築と管理に関する技術群。Dockerコンテナによる再現可能な環境管理、仮想マシンによるリソース分離、エッジデバイスへの推論デプロイメントを扱う",
            "children": [
              {
                "name": "Docker",
                "color": "#ec4899",
                "desc": "コンテナ型仮想化プラットフォーム。OS・ライブラリ・フレームワークの環境をDockerfileで定義しイメージとして配布する。DL環境の再現性確保とチーム間共有の標準ツール",
                "children": [
                  {
                    "name": "コンテナ型仮想化",
                    "color": "#ec4899",
                    "desc": "ホストOSのカーネルを共有し名前空間・cgroupsでプロセスを隔離する軽量仮想化。VMと比べ起動が秒単位で高速、オーバーヘッドが極めて小さい。マイクロサービスの基盤技術"
                  },
                  {
                    "name": "レジストリ",
                    "color": "#ec4899",
                    "desc": "Dockerイメージを保存・配布するリポジトリサービス。Docker Hub(公開)・ECR/GCR(クラウド)・Harbor(プライベート)等。NGC(NVIDIA)がDL最適化済みコンテナイメージを提供する"
                  },
                  {
                    "name": "OSレベル仮想化",
                    "color": "#ec4899",
                    "desc": "ホストOSのカーネルをコンテナ間で共有しユーザー空間のみを隔離する仮想化方式。ゲストOSが不要なためVMより軽量。Linux namespacesとcgroupsが技術的基盤"
                  }
                ]
              },
              {
                "name": "仮想マシン",
                "color": "#ec4899",
                "desc": "ハードウェアをソフトウェアでエミュレートして完全に独立したOS環境を提供する仮想化技術。コンテナより重いが完全なOS分離を実現し、異なるOS・カーネルの実行が可能",
                "children": [
                  {
                    "name": "ハイパーバイザー型仮想化",
                    "color": "#ec4899",
                    "desc": "ベアメタル(物理ハードウェア)上で直接動作するType1ハイパーバイザー(VMware ESXi・Xen・KVM)がVMを管理する方式。オーバーヘッドが小さくサーバー仮想化の標準"
                  },
                  {
                    "name": "ホスト型仮想化",
                    "color": "#ec4899",
                    "desc": "ホストOS上のアプリケーションとして動作するType2ハイパーバイザー(VirtualBox・VMware Workstation)がVMを管理する方式。導入が容易だがType1より性能オーバーヘッドが大きい"
                  },
                  {
                    "name": "ハードウェア仮想化",
                    "color": "#ec4899",
                    "desc": "CPUのIntel VT-x・AMD-V等のハードウェア支援機能を用いてVMの実行を高速化する技術。特権命令のトラップ処理をハードウェアで高速化しVMのオーバーヘッドを最小化する"
                  }
                ]
              },
              {
                "name": "エッジ・IoT",
                "color": "#ec4899",
                "desc": "データ発生源の近くで推論処理を実行するエッジコンピューティングの技術領域。低遅延・プライバシー保護・帯域節約が利点。モデル軽量化技術との組合せが推論デプロイの鍵",
                "children": [
                  {
                    "name": "エッジコンピューティング",
                    "color": "#ec4899",
                    "desc": "クラウドではなくデバイスの近く(エッジ)でデータ処理・推論を行うパラダイム。自動運転の即座な判断・工場の異常検知等のリアルタイム要件を満たし通信コストとプライバシーリスクを低減"
                  },
                  {
                    "name": "IoTデバイス",
                    "color": "#ec4899",
                    "desc": "Internet of Thingsの端末装置。カメラ・センサー・マイコンボード(Jetson・RPi等)。限られた計算資源(数MB〜数GBメモリ)でDL推論を実行するためモデル軽量化が必須"
                  },
                  {
                    "name": "ディスクI/O",
                    "color": "#ec4899",
                    "desc": "ストレージ(SSD/HDD)への読み書き速度。大規模データセットの学習時にデータ読込がボトルネックとなりうる。SSD使用・データのメモリキャッシュ・並列読込(num_workers)で最適化する"
                  }
                ]
              }
            ]
          }
        ]
      }
    ]
  },
  "abbr": {
    "MLE": "Maximum Likelihood Estimation（最尤推定）",
    "MAP": "Maximum A Posteriori estimation（最大事後確率推定）",
    "ELBO": "Evidence Lower BOund（変分下界 / エビデンス下界）",
    "KL": "Kullback-Leibler divergence（カルバック・ライブラー情報量）",
    "JS": "Jensen-Shannon divergence（ジェンセン・シャノン情報量）",
    "KLダイバージェンス": "Kullback-Leibler Divergence（カルバック・ライブラー情報量）",
    "JSダイバージェンス": "Jensen-Shannon Divergence（ジェンセン・シャノン情報量）",
    "PCA": "Principal Component Analysis（主成分分析）",
    "LSI": "Latent Semantic Indexing（潜在的意味インデキシング）",
    "SVM": "Support Vector Machine（サポートベクターマシン）",
    "kNN": "k-Nearest Neighbors（k最近傍法）",
    "k近傍法": "k-Nearest Neighbors",
    "LSH": "Locality-Sensitive Hashing（局所性鋭敏型ハッシュ）",
    "HNSW": "Hierarchical Navigable Small World（階層的小世界グラフ）",
    "AUC": "Area Under the Curve（曲線下面積）",
    "ROC曲線": "Receiver Operating Characteristic Curve",
    "PR曲線": "Precision-Recall Curve",
    "AP": "Average Precision（平均適合率）",
    "mAP": "mean Average Precision（平均AP）",
    "MSE": "Mean Squared Error（平均二乗誤差）",
    "RMSE": "Root Mean Squared Error（二乗平均平方根誤差）",
    "MAE": "Mean Absolute Error（平均絶対誤差）",
    "ReLU": "Rectified Linear Unit（正規化線形関数）",
    "Leaky ReLU": "Leaky Rectified Linear Unit",
    "GELU": "Gaussian Error Linear Unit（ガウス誤差線形関数）",
    "Softmax": "Softmax Function（ソフトマックス関数）",
    "SGD": "Stochastic Gradient Descent（確率的勾配降下法）",
    "GD / SGD": "Gradient Descent / Stochastic Gradient Descent",
    "NAG": "Nesterov Accelerated Gradient（ネステロフの加速勾配法）",
    "Momentum / NAG": "Momentum / Nesterov Accelerated Gradient",
    "AdaGrad / RMSProp": "Adaptive Gradient / Root Mean Square Propagation",
    "AdaGrad": "Adaptive Gradient（適応的勾配法）",
    "RMSProp": "Root Mean Square Propagation",
    "Adam": "Adaptive Moment Estimation（適応的モーメント推定）",
    "AdamW": "Adam with decoupled Weight decay",
    "CNN": "Convolutional Neural Network（畳み込みニューラルネットワーク）",
    "RNN": "Recurrent Neural Network（リカレントニューラルネットワーク）",
    "LSTM": "Long Short-Term Memory（長短期記憶）",
    "GRU": "Gated Recurrent Unit（ゲート付き回帰ユニット）",
    "BPTT法": "Backpropagation Through Time（通時的誤差逆伝播法）",
    "Seq2Seq": "Sequence to Sequence（系列変換モデル）",
    "ViT": "Vision Transformer",
    "BERT": "Bidirectional Encoder Representations from Transformers",
    "GPT": "Generative Pre-trained Transformer",
    "MLM": "Masked Language Model（マスク言語モデル）",
    "NSP": "Next Sentence Prediction（次文予測）",
    "CBOW": "Continuous Bag of Words（連続Bag-of-Words）",
    "VAE": "Variational Autoencoder（変分オートエンコーダ）",
    "GAN": "Generative Adversarial Network（敵対的生成ネットワーク）",
    "WGAN": "Wasserstein GAN",
    "DCGAN": "Deep Convolutional GAN",
    "CGAN": "Conditional GAN（条件付きGAN）",
    "DQN": "Deep Q-Network",
    "A3C": "Asynchronous Advantage Actor-Critic（非同期アドバンテージAC）",
    "TD学習": "Temporal Difference Learning（時間的差分学習）",
    "CAM": "Class Activation Mapping（クラス活性化マッピング）",
    "Grad-CAM": "Gradient-weighted Class Activation Mapping",
    "CAM / Grad-CAM": "Class Activation Mapping / Gradient-weighted CAM",
    "XAI": "Explainable Artificial Intelligence（説明可能AI）",
    "XAI(説明性)": "Explainable AI（説明可能AI）",
    "SHAP": "SHapley Additive exPlanations",
    "LIME": "Local Interpretable Model-agnostic Explanations",
    "YOLO": "You Only Look Once",
    "SSD": "Single Shot MultiBox Detector",
    "RPN": "Region Proposal Network（領域候補ネットワーク）",
    "FPN": "Feature Pyramid Network（特徴量ピラミッドネットワーク）",
    "IoU": "Intersection over Union（共通部分÷和集合）",
    "NMS": "Non-Maximum Suppression（非最大値抑制）",
    "FCOS": "Fully Convolutional One-Stage detector",
    "FCN": "Fully Convolutional Network（全畳み込みネットワーク）",
    "GAP": "Global Average Pooling（大域平均プーリング）",
    "ResNet": "Residual Network（残差ネットワーク）",
    "WideResNet": "Wide Residual Network",
    "MLP": "Multi-Layer Perceptron（多層パーセプトロン）",
    "EDA": "Easy Data Augmentation",
    "NLP": "Natural Language Processing（自然言語処理）",
    "GPU": "Graphics Processing Unit",
    "CUDA": "Compute Unified Device Architecture",
    "cuDNN": "CUDA Deep Neural Network library",
    "SIMT": "Single Instruction Multiple Threads",
    "TPU": "Tensor Processing Unit",
    "MXU": "Matrix Multiply Unit（行列演算ユニット）",
    "CPU": "Central Processing Unit（中央処理装置）",
    "SIMD": "Single Instruction Multiple Data",
    "MIMD": "Multiple Instruction Multiple Data",
    "PTQ": "Post Training Quantization（学習後量子化）",
    "QAT": "Quantization-Aware Training（量子化考慮学習）",
    "INT8": "8-bit Integer Quantization（8ビット整数量子化）",
    "GPU / CUDA": "Graphics Processing Unit / Compute Unified Device Architecture",
    "TPU / MXU": "Tensor Processing Unit / Matrix Multiply Unit",
    "CPU / SIMD / MIMD": "Central Processing Unit / Single Instruction Multiple Data / Multiple Instruction Multiple Data",
    "BN": "Batch Normalization（バッチ正規化）",
    "LN": "Layer Normalization（レイヤー正規化）",
    "GN": "Group Normalization（グループ正規化）",
    "IN": "Instance Normalization（インスタンス正規化）",
    "Faster R-CNN": "Faster Region-based Convolutional Neural Network",
    "Mask R-CNN": "Mask Region-based Convolutional Neural Network",
    "RoIプーリング": "Region of Interest Pooling",
    "RoIAlign": "Region of Interest Align",
    "Elastic Net": "Elastic Net Regularization（弾性ネット正則化）",
    "t-SNE": "t-distributed Stochastic Neighbor Embedding",
    "U-Net": "U-shaped Network（U字型ネットワーク）",
    "Focal Loss": "Focal Loss for Dense Object Detection",
    "MixUp": "MixUp: Beyond Empirical Risk Minimization",
    "MixMatch": "MixMatch: A Holistic Approach to Semi-Supervised Learning",
    "RandAugment": "RandAugment: Practical Automated Data Augmentation",
    "Dice係数": "Dice-Sørensen Coefficient（ダイス係数）",
    "Jaccard係数": "Jaccard Index / Intersection over Union",
    "IoTデバイス": "Internet of Things Device",
    "AllReduce": "All-Reduce Collective Communication",
    "Siamese network": "Siamese Neural Network（シャムネットワーク）",
    "Triplet network": "Triplet Neural Network（トリプレットネットワーク）",
    "Triplet loss": "Triplet Margin Loss（トリプレットマージン損失）",
    "Contrastive loss": "Contrastive Loss（対照損失）",
    "Contrastive learning": "Contrastive Learning（対照学習）",
    "Contrastive学習": "Contrastive Learning（対照学習）",
    "skip-gram": "Skip-gram Model",
    "ActNorm": "Activation Normalization（活性化正規化）",
    "InfoMax原理": "Information Maximization Principle（情報量最大化原理）",
    "F値": "F-measure / F-score",
    "Affine層": "Affine Layer（アフィン変換層）",
    "Sigmoid function": "Sigmoid Activation Function（シグモイド活性化関数）",
    "Pathological Curvature": "Pathological Curvature（病的曲率）",
    "Docker": "Docker Container Platform",
    "NumPy": "Numerical Python",
    "PyTorch": "PyTorch Deep Learning Framework（Meta / Facebook AI Research）",
    "TensorFlow": "TensorFlow Machine Learning Framework（Google）",
    "DataLoader": "torch.utils.data.DataLoader",
    "nn.Module": "torch.nn.Module",
    "tf.keras": "TensorFlow Keras API",
    "GradientTape": "tf.GradientTape（TensorFlow自動微分API）",
    "autograd": "Automatic Gradient Computation（自動勾配計算）",
    "ndarray": "N-dimensional Array（N次元配列）"
  }
}